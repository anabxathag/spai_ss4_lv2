{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "O3kuVrBnOZWu",
        "ZQWd8Q5JK5MG",
        "-tcSHmZWK-6P",
        "OugEcD33Bdgh"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# load data"
      ],
      "metadata": {
        "id": "IZ9kOCk7OWYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "id": "fa1UxK94NAXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir smc\n",
        "!unzip smc -d smc"
      ],
      "metadata": {
        "id": "9FxsQ0C55RyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir smc_test\n",
        "# !unzip smc_test.zip -d smc_test"
      ],
      "metadata": {
        "id": "N85bJ4FmNN_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir 400223-Art/dataset\n",
        "# !mv smc/ 400223-Art/dataset\n",
        "# !mv smc_test/ 400223-Art/dataset"
      ],
      "metadata": {
        "id": "JM_6OkK-Nt-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# create new format data"
      ],
      "metadata": {
        "id": "O3kuVrBnOZWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "qxvWv_DjSq9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def count_files_in_directory(directory_path):\n",
        "    # List all items in the directory\n",
        "    all_items = os.listdir(directory_path)\n",
        "    # Filter out only files (not directories)\n",
        "    files = [item for item in all_items if os.path.isfile(os.path.join(directory_path, item))]\n",
        "    return len(files)\n",
        "\n",
        "# Example usage\n",
        "## train 14448\n",
        "count_train = 0\n",
        "directory_path = '/lustrefs/disk/project/lt900103-ai24e3/400223-Art/dataset/smc_trans/train/FFC'\n",
        "print(f\"Number of files: {count_files_in_directory(directory_path)}\")\n",
        "count_train += count_files_in_directory(directory_path)\n",
        "directory_path = '/lustrefs/disk/project/lt900103-ai24e3/400223-Art/dataset/smc_trans/train/FFS'\n",
        "print(f\"Number of files: {count_files_in_directory(directory_path)}\")\n",
        "count_train += count_files_in_directory(directory_path)\n",
        "directory_path = '/lustrefs/disk/project/lt900103-ai24e3/400223-Art/dataset/smc_trans/train/HCC'\n",
        "print(f\"Number of files: {count_files_in_directory(directory_path)}\")\n",
        "count_train += count_files_in_directory(directory_path)\n",
        "directory_path = '/lustrefs/disk/project/lt900103-ai24e3/400223-Art/dataset/smc_trans/train/cyst'\n",
        "print(f\"Number of files: {count_files_in_directory(directory_path)}\")\n",
        "count_train += count_files_in_directory(directory_path)\n",
        "directory_path = '/lustrefs/disk/project/lt900103-ai24e3/400223-Art/dataset/smc_trans/train/hemangioma'\n",
        "print(f\"Number of files: {count_files_in_directory(directory_path)}\")\n",
        "count_train += count_files_in_directory(directory_path)\n",
        "directory_path = '/lustrefs/disk/project/lt900103-ai24e3/400223-Art/dataset/smc_trans/train/dysplastic'\n",
        "print(f\"Number of files: {count_files_in_directory(directory_path)}\")\n",
        "count_train += count_files_in_directory(directory_path)\n",
        "directory_path = '/lustrefs/disk/project/lt900103-ai24e3/400223-Art/dataset/smc_trans/train/CCA'\n",
        "print(f\"Number of files: {count_files_in_directory(directory_path)}\")\n",
        "count_train += count_files_in_directory(directory_path)\n",
        "directory_path = '/lustrefs/disk/project/lt900103-ai24e3/400223-Art/dataset/smc_trans/train/background'\n",
        "print(f\"Number of files: {count_files_in_directory(directory_path)}\")\n",
        "count_train += count_files_in_directory(directory_path)\n",
        "print(f\"Number train: {(count_train)}\\n\")\n",
        "\n",
        "## val 4898\n",
        "count_test = 0\n",
        "directory_path = '/lustrefs/disk/project/lt900103-ai24e3/400223-Art/dataset/smc_trans/val/FFC'\n",
        "print(f\"Number of files: {count_files_in_directory(directory_path)}\")\n",
        "count_test += count_files_in_directory(directory_path)\n",
        "directory_path = '/lustrefs/disk/project/lt900103-ai24e3/400223-Art/dataset/smc_trans/val/FFS'\n",
        "print(f\"Number of files: {count_files_in_directory(directory_path)}\")\n",
        "count_test += count_files_in_directory(directory_path)\n",
        "directory_path = '/lustrefs/disk/project/lt900103-ai24e3/400223-Art/dataset/smc_trans/val/HCC'\n",
        "print(f\"Number of files: {count_files_in_directory(directory_path)}\")\n",
        "count_test += count_files_in_directory(directory_path)\n",
        "directory_path = '/lustrefs/disk/project/lt900103-ai24e3/400223-Art/dataset/smc_trans/val/cyst'\n",
        "print(f\"Number of files: {count_files_in_directory(directory_path)}\")\n",
        "count_test += count_files_in_directory(directory_path)\n",
        "directory_path = '/lustrefs/disk/project/lt900103-ai24e3/400223-Art/dataset/smc_trans/val/hemangioma'\n",
        "print(f\"Number of files: {count_files_in_directory(directory_path)}\")\n",
        "count_test += count_files_in_directory(directory_path)\n",
        "directory_path = '/lustrefs/disk/project/lt900103-ai24e3/400223-Art/dataset/smc_trans/val/dysplastic'\n",
        "print(f\"Number of files: {count_files_in_directory(directory_path)}\")\n",
        "count_test += count_files_in_directory(directory_path)\n",
        "directory_path = '/lustrefs/disk/project/lt900103-ai24e3/400223-Art/dataset/smc_trans/val/CCA'\n",
        "print(f\"Number of files: {count_files_in_directory(directory_path)}\")\n",
        "count_test += count_files_in_directory(directory_path)\n",
        "directory_path = '/lustrefs/disk/project/lt900103-ai24e3/400223-Art/dataset/smc_trans/val/background'\n",
        "print(f\"Number of files: {count_files_in_directory(directory_path)}\")\n",
        "count_test += count_files_in_directory(directory_path)\n",
        "print(f\"Number test: {(count_test)}\\n\")"
      ],
      "metadata": {
        "id": "FR5PMbKNUIw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = '/lustrefs/disk/project/lt900103-ai24e3/400223-Art/dataset/smc/train/images/100010.jpg'\n",
        "image = cv2.imread(image_path, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(image, cmap='grey')\n",
        "plt.show()\n",
        "image.shape"
      ],
      "metadata": {
        "id": "A-30oogJSyjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir 400223-Art/dataset/smc_trans\n",
        "!mkdir 400223-Art/dataset/smc_trans/train\n",
        "!mkdir 400223-Art/dataset/smc_trans/train/background\n",
        "!mkdir 400223-Art/dataset/smc_trans/train/FFC\n",
        "!mkdir 400223-Art/dataset/smc_trans/train/FFS\n",
        "!mkdir 400223-Art/dataset/smc_trans/train/HCC\n",
        "!mkdir 400223-Art/dataset/smc_trans/train/cyst\n",
        "!mkdir 400223-Art/dataset/smc_trans/train/hemangioma\n",
        "!mkdir 400223-Art/dataset/smc_trans/train/dysplastic\n",
        "!mkdir 400223-Art/dataset/smc_trans/train/CCA\n",
        "!mkdir 400223-Art/dataset/smc_trans/val\n",
        "!mkdir 400223-Art/dataset/smc_trans/val/background\n",
        "!mkdir 400223-Art/dataset/smc_trans/val/FFC\n",
        "!mkdir 400223-Art/dataset/smc_trans/val/FFS\n",
        "!mkdir 400223-Art/dataset/smc_trans/val/HCC\n",
        "!mkdir 400223-Art/dataset/smc_trans/val/cyst\n",
        "!mkdir 400223-Art/dataset/smc_trans/val/hemangioma\n",
        "!mkdir 400223-Art/dataset/smc_trans/val/dysplastic\n",
        "!mkdir 400223-Art/dataset/smc_trans/val/CCA"
      ],
      "metadata": {
        "id": "IxPv3InTiNYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def count_files_in_directory(directory_path):\n",
        "    # List all items in the directory\n",
        "    all_items = os.listdir(directory_path)\n",
        "    # Filter out only files (not directories)\n",
        "    files = [item for item in all_items if os.path.isfile(os.path.join(directory_path, item))]\n",
        "    return len(files)\n",
        "\n",
        "# Example usage\n",
        "directory_path = '/lustrefs/disk/project/lt900103-ai24e3/400223-Art/dataset/smc/train/images'\n",
        "print(f\"Number of files: {count_files_in_directory(directory_path)}\")\n",
        "directory_path = '/lustrefs/disk/project/lt900103-ai24e3/400223-Art/dataset/smc/train/labels'\n",
        "print(f\"Number of files: {count_files_in_directory(directory_path)}\")\n",
        "directory_path = '/lustrefs/disk/project/lt900103-ai24e3/400223-Art/dataset/smc/val/images'\n",
        "print(f\"Number of files: {count_files_in_directory(directory_path)}\")\n",
        "directory_path = '/lustrefs/disk/project/lt900103-ai24e3/400223-Art/dataset/smc/val/labels'\n",
        "print(f\"Number of files: {count_files_in_directory(directory_path)}\\n\")\n",
        "directory_path = '/lustrefs/disk/project/lt900103-ai24e3/400223-Art/dataset/smc_test/images'\n",
        "print(f\"Number of files: {count_files_in_directory(directory_path)}\")"
      ],
      "metadata": {
        "id": "k5rFf-YHDYV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define the paths\n",
        "\n",
        "dataset_path = '/lustrefs/disk/project/lt900103-ai24e3/400223-Art/dataset/smc/'\n",
        "output_path = '/lustrefs/disk/project/lt900103-ai24e3/400223-Art/dataset/smc_trans'\n",
        "\n",
        "train_images_path = os.path.join(dataset_path, 'train', 'images')\n",
        "train_labels_path = os.path.join(dataset_path, 'train', 'labels')\n",
        "val_images_path = os.path.join(dataset_path, 'val', 'images')\n",
        "val_labels_path = os.path.join(dataset_path, 'val', 'labels')\n",
        "\n",
        "# Define label-to-directory mapping\n",
        "label_mapping = {\n",
        "    0: 'FFC',\n",
        "    1: 'FFS',\n",
        "    2: 'HCC',\n",
        "    3: 'cyst',\n",
        "    4: 'hemangioma',\n",
        "    5: 'dysplastic',\n",
        "    6: 'CCA'\n",
        "}\n",
        "\n",
        "def process_images_and_labels(images_path, labels_path, label_mapping):\n",
        "    image_files = [f for f in os.listdir(images_path) if f.endswith('.jpg')]\n",
        "\n",
        "    for image_file in tqdm(image_files):\n",
        "        label_file = image_file.replace('.jpg', '.txt')\n",
        "        label_path = os.path.join(labels_path, label_file)\n",
        "\n",
        "        if os.path.exists(label_path):\n",
        "            with open(label_path, 'r') as file:\n",
        "                content = file.read().strip()\n",
        "                if content:\n",
        "                    label = int(content.split()[0])\n",
        "                else:\n",
        "                    label = 'NaN'\n",
        "        else:\n",
        "            label = 'NaN'\n",
        "\n",
        "        dest_label = label_mapping.get(label, 'background')\n",
        "        kind = 'train' if 'train' in labels_path else 'val'\n",
        "\n",
        "        src_image_path = os.path.join(images_path, image_file)\n",
        "        dest_image_path = os.path.join(output_path, kind, dest_label, image_file)\n",
        "\n",
        "        image = cv2.imread(src_image_path, cv2.IMREAD_COLOR)\n",
        "        if image is not None:\n",
        "            image = cv2.resize(image, (512, 512))\n",
        "            cv2.imwrite(dest_image_path, image)\n",
        "\n",
        "# Process train dataset\n",
        "process_images_and_labels(train_images_path, train_labels_path, label_mapping)\n",
        "\n",
        "# Process val dataset\n",
        "process_images_and_labels(val_images_path, val_labels_path, label_mapping)\n",
        "\n",
        "print(\"Dataset restructuring complete.\")"
      ],
      "metadata": {
        "id": "JMfmBG02bSvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# function for VIT"
      ],
      "metadata": {
        "id": "ZQWd8Q5JK5MG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "train_transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "        transforms.Resize((32, 32)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomResizedCrop((32, 32), scale=(0.8, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.ImageFolder(root='/lustrefs/disk/project/lt900103-ai24e3/400223-Art/dataset/smc_trans/train', transform=train_transform)\n",
        "trainset"
      ],
      "metadata": {
        "id": "H76XRCEbPC4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16,\n",
        "                                            shuffle=True, num_workers=2)\n",
        "trainloader"
      ],
      "metadata": {
        "id": "eDB8RGkyKAmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        image = cv2.imread(img_path, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, img_name  # Return image and filename\n",
        "\n",
        "# Define the path to your test images\n",
        "test_images_path = '/lustrefs/disk/project/lt900103-ai24e3/400223-Art/dataset/smc_test/images'\n",
        "\n",
        "# Define your transforms (if any)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((512, 512)),  # Resize the images to 512x512\n",
        "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
        "])\n",
        "\n",
        "# Create an instance of the dataset\n",
        "test_dataset = CustomImageDataset(test_images_path, transform=transform)\n",
        "\n",
        "# Create a DataLoader\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# # Example usage: Iterate over the test_loader\n",
        "# for images, filenames in test_loader:\n",
        "#     # images: a batch of images\n",
        "#     # filenames: the corresponding filenames of the images in the batch\n",
        "#     print(filenames)\n"
      ],
      "metadata": {
        "id": "IN_XNmQCyKh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader"
      ],
      "metadata": {
        "id": "J410UUqk19Mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nzfs_jmYyKdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wfNInIVlyKYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## data"
      ],
      "metadata": {
        "id": "dQeYuDCHK9MP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def prepare_data(batch_size=16, num_workers=2, train_sample_size=None, test_sample_size=None):\n",
        "    train_transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "        transforms.Resize((32, 32)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomResizedCrop((32, 32), scale=(0.8, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                            download=True, transform=train_transform)\n",
        "    if train_sample_size is not None:\n",
        "        # Randomly sample a subset of the training set\n",
        "        indices = torch.randperm(len(trainset))[:train_sample_size]\n",
        "        trainset = torch.utils.data.Subset(trainset, indices)\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                            shuffle=True, num_workers=num_workers)\n",
        "\n",
        "    test_transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "        transforms.Resize((32, 32)),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                        download=True, transform=test_transform)\n",
        "    if test_sample_size is not None:\n",
        "        # Randomly sample a subset of the test set\n",
        "        indices = torch.randperm(len(testset))[:test_sample_size]\n",
        "        testset = torch.utils.data.Subset(testset, indices)\n",
        "\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                            shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    classes = ('FFC', 'FFS', 'HCC', 'cyst', 'hemangioma', 'dysplastic', 'CCA', 'background')\n",
        "    return trainloader, testloader, classes"
      ],
      "metadata": {
        "id": "38f-JqpWNt8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vit"
      ],
      "metadata": {
        "id": "-tcSHmZWK-6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class NewGELUActivation(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see\n",
        "    the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415\n",
        "\n",
        "    Taken from https://github.com/huggingface/transformers/blob/main/src/transformers/activations.py\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, input):\n",
        "        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
        "\n",
        "class PatchEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Convert the image into patches and then project them into a vector space.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.image_size = config[\"image_size\"]\n",
        "        self.patch_size = config[\"patch_size\"]\n",
        "        self.num_channels = config[\"num_channels\"]\n",
        "        self.hidden_size = config[\"hidden_size\"]\n",
        "        # Calculate the number of patches from the image size and patch size\n",
        "        self.num_patches = (self.image_size // self.patch_size) ** 2\n",
        "        # Create a projection layer to convert the image into patches\n",
        "        # The layer projects each patch into a vector of size hidden_size\n",
        "        self.projection = nn.Conv2d(self.num_channels, self.hidden_size, kernel_size=self.patch_size, stride=self.patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch_size, num_channels, image_size, image_size) -> (batch_size, num_patches, hidden_size)\n",
        "        x = self.projection(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Combine the patch embeddings with the class token and position embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.patch_embeddings = PatchEmbeddings(config)\n",
        "        # Create a learnable [CLS] token\n",
        "        # Similar to BERT, the [CLS] token is added to the beginning of the input sequence\n",
        "        # and is used to classify the entire sequence\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, config[\"hidden_size\"]))\n",
        "        # Create position embeddings for the [CLS] token and the patch embeddings\n",
        "        # Add 1 to the sequence length for the [CLS] token\n",
        "        self.position_embeddings = \\\n",
        "            nn.Parameter(torch.randn(1, self.patch_embeddings.num_patches + 1, config[\"hidden_size\"]))\n",
        "        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embeddings(x)\n",
        "        batch_size, _, _ = x.size()\n",
        "        # Expand the [CLS] token to the batch size\n",
        "        # (1, 1, hidden_size) -> (batch_size, 1, hidden_size)\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "        # Concatenate the [CLS] token to the beginning of the input sequence\n",
        "        # This results in a sequence length of (num_patches + 1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.position_embeddings\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class AttentionHead(nn.Module):\n",
        "    \"\"\"\n",
        "    A single attention head.\n",
        "    This module is used in the MultiHeadAttention module.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, attention_head_size, dropout, bias=True):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.attention_head_size = attention_head_size\n",
        "        # Create the query, key, and value projection layers\n",
        "        self.query = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
        "        self.key = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
        "        self.value = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Project the input into query, key, and value\n",
        "        # The same input is used to generate the query, key, and value,\n",
        "        # so it's usually called self-attention.\n",
        "        # (batch_size, sequence_length, hidden_size) -> (batch_size, sequence_length, attention_head_size)\n",
        "        query = self.query(x)\n",
        "        key = self.key(x)\n",
        "        value = self.value(x)\n",
        "        # Calculate the attention scores\n",
        "        # softmax(Q*K.T/sqrt(head_size))*V\n",
        "        attention_scores = torch.matmul(query, key.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "        # Calculate the attention output\n",
        "        attention_output = torch.matmul(attention_probs, value)\n",
        "        return (attention_output, attention_probs)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head attention module.\n",
        "    This module is used in the TransformerEncoder module.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.hidden_size = config[\"hidden_size\"]\n",
        "        self.num_attention_heads = config[\"num_attention_heads\"]\n",
        "        # The attention head size is the hidden size divided by the number of attention heads\n",
        "        self.attention_head_size = self.hidden_size // self.num_attention_heads\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "        # Whether or not to use bias in the query, key, and value projection layers\n",
        "        self.qkv_bias = config[\"qkv_bias\"]\n",
        "        # Create a list of attention heads\n",
        "        self.heads = nn.ModuleList([])\n",
        "        for _ in range(self.num_attention_heads):\n",
        "            head = AttentionHead(\n",
        "                self.hidden_size,\n",
        "                self.attention_head_size,\n",
        "                config[\"attention_probs_dropout_prob\"],\n",
        "                self.qkv_bias\n",
        "            )\n",
        "            self.heads.append(head)\n",
        "        # Create a linear layer to project the attention output back to the hidden size\n",
        "        # In most cases, all_head_size and hidden_size are the same\n",
        "        self.output_projection = nn.Linear(self.all_head_size, self.hidden_size)\n",
        "        self.output_dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
        "\n",
        "    def forward(self, x, output_attentions=False):\n",
        "        # Calculate the attention output for each attention head\n",
        "        attention_outputs = [head(x) for head in self.heads]\n",
        "        # Concatenate the attention outputs from each attention head\n",
        "        attention_output = torch.cat([attention_output for attention_output, _ in attention_outputs], dim=-1)\n",
        "        # Project the concatenated attention output back to the hidden size\n",
        "        attention_output = self.output_projection(attention_output)\n",
        "        attention_output = self.output_dropout(attention_output)\n",
        "        # Return the attention output and the attention probabilities (optional)\n",
        "        if not output_attentions:\n",
        "            return (attention_output, None)\n",
        "        else:\n",
        "            attention_probs = torch.stack([attention_probs for _, attention_probs in attention_outputs], dim=1)\n",
        "            return (attention_output, attention_probs)\n",
        "\n",
        "class FasterMultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head attention module with some optimizations.\n",
        "    All the heads are processed simultaneously with merged query, key, and value projections.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.hidden_size = config[\"hidden_size\"]\n",
        "        self.num_attention_heads = config[\"num_attention_heads\"]\n",
        "        # The attention head size is the hidden size divided by the number of attention heads\n",
        "        self.attention_head_size = self.hidden_size // self.num_attention_heads\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "        # Whether or not to use bias in the query, key, and value projection layers\n",
        "        self.qkv_bias = config[\"qkv_bias\"]\n",
        "        # Create a linear layer to project the query, key, and value\n",
        "        self.qkv_projection = nn.Linear(self.hidden_size, self.all_head_size * 3, bias=self.qkv_bias)\n",
        "        self.attn_dropout = nn.Dropout(config[\"attention_probs_dropout_prob\"])\n",
        "        # Create a linear layer to project the attention output back to the hidden size\n",
        "        # In most cases, all_head_size and hidden_size are the same\n",
        "        self.output_projection = nn.Linear(self.all_head_size, self.hidden_size)\n",
        "        self.output_dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
        "\n",
        "    def forward(self, x, output_attentions=False):\n",
        "        # Project the query, key, and value\n",
        "        # (batch_size, sequence_length, hidden_size) -> (batch_size, sequence_length, all_head_size * 3)\n",
        "        qkv = self.qkv_projection(x)\n",
        "        # Split the projected query, key, and value into query, key, and value\n",
        "        # (batch_size, sequence_length, all_head_size * 3) -> (batch_size, sequence_length, all_head_size)\n",
        "        query, key, value = torch.chunk(qkv, 3, dim=-1)\n",
        "        # Resize the query, key, and value to (batch_size, num_attention_heads, sequence_length, attention_head_size)\n",
        "        batch_size, sequence_length, _ = query.size()\n",
        "        query = query.view(batch_size, sequence_length, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
        "        key = key.view(batch_size, sequence_length, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
        "        value = value.view(batch_size, sequence_length, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
        "        # Calculate the attention scores\n",
        "        # softmax(Q*K.T/sqrt(head_size))*V\n",
        "        attention_scores = torch.matmul(query, key.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
        "        attention_probs = self.attn_dropout(attention_probs)\n",
        "        # Calculate the attention output\n",
        "        attention_output = torch.matmul(attention_probs, value)\n",
        "        # Resize the attention output\n",
        "        # from (batch_size, num_attention_heads, sequence_length, attention_head_size)\n",
        "        # To (batch_size, sequence_length, all_head_size)\n",
        "        attention_output = attention_output.transpose(1, 2) \\\n",
        "                                           .contiguous() \\\n",
        "                                           .view(batch_size, sequence_length, self.all_head_size)\n",
        "        # Project the attention output back to the hidden size\n",
        "        attention_output = self.output_projection(attention_output)\n",
        "        attention_output = self.output_dropout(attention_output)\n",
        "        # Return the attention output and the attention probabilities (optional)\n",
        "        if not output_attentions:\n",
        "            return (attention_output, None)\n",
        "        else:\n",
        "            return (attention_output, attention_probs)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    A multi-layer perceptron module.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense_1 = nn.Linear(config[\"hidden_size\"], config[\"intermediate_size\"])\n",
        "        self.activation = NewGELUActivation()\n",
        "        self.dense_2 = nn.Linear(config[\"intermediate_size\"], config[\"hidden_size\"])\n",
        "        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dense_1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dense_2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"\n",
        "    A single transformer block.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.use_faster_attention = config.get(\"use_faster_attention\", False)\n",
        "        if self.use_faster_attention:\n",
        "            self.attention = FasterMultiHeadAttention(config)\n",
        "        else:\n",
        "            self.attention = MultiHeadAttention(config)\n",
        "        self.layernorm_1 = nn.LayerNorm(config[\"hidden_size\"])\n",
        "        self.mlp = MLP(config)\n",
        "        self.layernorm_2 = nn.LayerNorm(config[\"hidden_size\"])\n",
        "\n",
        "    def forward(self, x, output_attentions=False):\n",
        "        # Self-attention\n",
        "        attention_output, attention_probs = \\\n",
        "            self.attention(self.layernorm_1(x), output_attentions=output_attentions)\n",
        "        # Skip connection\n",
        "        x = x + attention_output\n",
        "        # Feed-forward network\n",
        "        mlp_output = self.mlp(self.layernorm_2(x))\n",
        "        # Skip connection\n",
        "        x = x + mlp_output\n",
        "        # Return the transformer block's output and the attention probabilities (optional)\n",
        "        if not output_attentions:\n",
        "            return (x, None)\n",
        "        else:\n",
        "            return (x, attention_probs)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The transformer encoder module.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # Create a list of transformer blocks\n",
        "        self.blocks = nn.ModuleList([])\n",
        "        for _ in range(config[\"num_hidden_layers\"]):\n",
        "            block = Block(config)\n",
        "            self.blocks.append(block)\n",
        "\n",
        "    def forward(self, x, output_attentions=False):\n",
        "        # Calculate the transformer block's output for each block\n",
        "        all_attentions = []\n",
        "        for block in self.blocks:\n",
        "            x, attention_probs = block(x, output_attentions=output_attentions)\n",
        "            if output_attentions:\n",
        "                all_attentions.append(attention_probs)\n",
        "        # Return the encoder's output and the attention probabilities (optional)\n",
        "        if not output_attentions:\n",
        "            return (x, None)\n",
        "        else:\n",
        "            return (x, all_attentions)\n",
        "\n",
        "class ViTForClassfication(nn.Module):\n",
        "    \"\"\"\n",
        "    The ViT model for classification.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.image_size = config[\"image_size\"]\n",
        "        self.hidden_size = config[\"hidden_size\"]\n",
        "        self.num_classes = config[\"num_classes\"]\n",
        "        # Create the embedding module\n",
        "        self.embedding = Embeddings(config)\n",
        "        # Create the transformer encoder module\n",
        "        self.encoder = Encoder(config)\n",
        "        # Create a linear layer to project the encoder's output to the number of classes\n",
        "        self.classifier = nn.Linear(self.hidden_size, self.num_classes)\n",
        "        # Initialize the weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def forward(self, x, output_attentions=False):\n",
        "        # Calculate the embedding output\n",
        "        embedding_output = self.embedding(x)\n",
        "        # Calculate the encoder's output\n",
        "        encoder_output, all_attentions = self.encoder(embedding_output, output_attentions=output_attentions)\n",
        "        # Calculate the logits, take the [CLS] token's output as features for classification\n",
        "        logits = self.classifier(encoder_output[:, 0, :])\n",
        "        # Return the logits and the attention probabilities (optional)\n",
        "        if not output_attentions:\n",
        "            return (logits, None)\n",
        "        else:\n",
        "            return (logits, all_attentions)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=self.config[\"initializer_range\"])\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "        elif isinstance(module, Embeddings):\n",
        "            module.position_embeddings.data = nn.init.trunc_normal_(\n",
        "                module.position_embeddings.data.to(torch.float32),\n",
        "                mean=0.0,\n",
        "                std=self.config[\"initializer_range\"],\n",
        "            ).to(module.position_embeddings.dtype)\n",
        "\n",
        "            module.cls_token.data = nn.init.trunc_normal_(\n",
        "                module.cls_token.data.to(torch.float32),\n",
        "                mean=0.0,\n",
        "                std=self.config[\"initializer_range\"],\n",
        "            ).to(module.cls_token.dtype)"
      ],
      "metadata": {
        "id": "7_ICWFoK5RuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## utils"
      ],
      "metadata": {
        "id": "p4CYafngLchh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json, os, math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# from vit import ViTForClassfication\n",
        "\n",
        "def save_experiment(experiment_name, config, model, train_losses, test_losses, accuracies, base_dir=\"experiments\"):\n",
        "    outdir = os.path.join(base_dir, experiment_name)\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "\n",
        "    # Save the config\n",
        "    configfile = os.path.join(outdir, 'config.json')\n",
        "    with open(configfile, 'w') as f:\n",
        "        json.dump(config, f, sort_keys=True, indent=4)\n",
        "\n",
        "    # Save the metrics\n",
        "    jsonfile = os.path.join(outdir, 'metrics.json')\n",
        "    with open(jsonfile, 'w') as f:\n",
        "        data = {\n",
        "            'train_losses': train_losses,\n",
        "            'test_losses': test_losses,\n",
        "            'accuracies': accuracies,\n",
        "        }\n",
        "        json.dump(data, f, sort_keys=True, indent=4)\n",
        "\n",
        "    # Save the model\n",
        "    save_checkpoint(experiment_name, model, \"final\", base_dir=base_dir)\n",
        "\n",
        "\n",
        "def save_checkpoint(experiment_name, model, epoch, base_dir=\"experiments\"):\n",
        "    outdir = os.path.join(base_dir, experiment_name)\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    cpfile = os.path.join(outdir, f'model_{epoch}.pt')\n",
        "    torch.save(model.state_dict(), cpfile)\n",
        "\n",
        "\n",
        "def load_experiment(experiment_name, checkpoint_name=\"model_final.pt\", base_dir=\"experiments\"):\n",
        "    outdir = os.path.join(base_dir, experiment_name)\n",
        "    # Load the config\n",
        "    configfile = os.path.join(outdir, 'config.json')\n",
        "    with open(configfile, 'r') as f:\n",
        "        config = json.load(f)\n",
        "    # Load the metrics\n",
        "    jsonfile = os.path.join(outdir, 'metrics.json')\n",
        "    with open(jsonfile, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    train_losses = data['train_losses']\n",
        "    test_losses = data['test_losses']\n",
        "    accuracies = data['accuracies']\n",
        "    # Load the model\n",
        "    model = ViTForClassfication(config)\n",
        "    cpfile = os.path.join(outdir, checkpoint_name)\n",
        "    model.load_state_dict(torch.load(cpfile))\n",
        "    return config, model, train_losses, test_losses, accuracies\n",
        "\n",
        "\n",
        "def visualize_images():\n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                            download=True)\n",
        "    classes = ('FFC', 'FFS', 'HCC', 'cyst', 'hemangioma', 'dysplastic', 'CCA', 'background')\n",
        "    # Pick 30 samples randomly\n",
        "    indices = torch.randperm(len(trainset))[:30]\n",
        "    images = [np.asarray(trainset[i][0]) for i in indices]\n",
        "    labels = [trainset[i][1] for i in indices]\n",
        "    # Visualize the images using matplotlib\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "    for i in range(30):\n",
        "        ax = fig.add_subplot(6, 5, i+1, xticks=[], yticks=[])\n",
        "        ax.imshow(images[i])\n",
        "        ax.set_title(classes[labels[i]])\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def visualize_attention(model, output=None, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Visualize the attention maps of the first 4 images.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    # Load random images\n",
        "    num_images = 30\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True)\n",
        "    classes = ('FFC', 'FFS', 'HCC', 'cyst', 'hemangioma', 'dysplastic', 'CCA', 'background')\n",
        "    # Pick 30 samples randomly\n",
        "    indices = torch.randperm(len(testset))[:num_images]\n",
        "    raw_images = [np.asarray(testset[i][0]) for i in indices]\n",
        "    labels = [testset[i][1] for i in indices]\n",
        "    # Convert the images to tensors\n",
        "    test_transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "        transforms.Resize((32, 32)),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "    images = torch.stack([test_transform(image) for image in raw_images])\n",
        "    # Move the images to the device\n",
        "    images = images.to(device)\n",
        "    model = model.to(device)\n",
        "    # Get the attention maps from the last block\n",
        "    logits, attention_maps = model(images, output_attentions=True)\n",
        "    # Get the predictions\n",
        "    predictions = torch.argmax(logits, dim=1)\n",
        "    # Concatenate the attention maps from all blocks\n",
        "    attention_maps = torch.cat(attention_maps, dim=1)\n",
        "    # select only the attention maps of the CLS token\n",
        "    attention_maps = attention_maps[:, :, 0, 1:]\n",
        "    # Then average the attention maps of the CLS token over all the heads\n",
        "    attention_maps = attention_maps.mean(dim=1)\n",
        "    # Reshape the attention maps to a square\n",
        "    num_patches = attention_maps.size(-1)\n",
        "    size = int(math.sqrt(num_patches))\n",
        "    attention_maps = attention_maps.view(-1, size, size)\n",
        "    # Resize the map to the size of the image\n",
        "    attention_maps = attention_maps.unsqueeze(1)\n",
        "    attention_maps = F.interpolate(attention_maps, size=(32, 32), mode='bilinear', align_corners=False)\n",
        "    attention_maps = attention_maps.squeeze(1)\n",
        "    # Plot the images and the attention maps\n",
        "    fig = plt.figure(figsize=(20, 10))\n",
        "    mask = np.concatenate([np.ones((32, 32)), np.zeros((32, 32))], axis=1)\n",
        "    for i in range(num_images):\n",
        "        ax = fig.add_subplot(6, 5, i+1, xticks=[], yticks=[])\n",
        "        img = np.concatenate((raw_images[i], raw_images[i]), axis=1)\n",
        "        ax.imshow(img)\n",
        "        # Mask out the attention map of the left image\n",
        "        extended_attention_map = np.concatenate((np.zeros((32, 32)), attention_maps[i].cpu()), axis=1)\n",
        "        extended_attention_map = np.ma.masked_where(mask==1, extended_attention_map)\n",
        "        ax.imshow(extended_attention_map, alpha=0.5, cmap='jet')\n",
        "        # Show the ground truth and the prediction\n",
        "        gt = classes[labels[i]]\n",
        "        pred = classes[predictions[i]]\n",
        "        ax.set_title(f\"gt: {gt} / pred: {pred}\", color=(\"green\" if gt==pred else \"red\"))\n",
        "    if output is not None:\n",
        "        plt.savefig(output)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "L57tTVDQLcZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train"
      ],
      "metadata": {
        "id": "HkanS7-ZLw9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "\n",
        "# from utils import save_experiment, save_checkpoint\n",
        "# from data import prepare_data\n",
        "# from vit import ViTForClassfication\n",
        "\n",
        "config = {\n",
        "    \"patch_size\": 4,  # Input image size: 32x32 -> 8x8 patches\n",
        "    \"hidden_size\": 48,\n",
        "    \"num_hidden_layers\": 4,\n",
        "    \"num_attention_heads\": 4,\n",
        "    \"intermediate_size\": 4 * 48, # 4 * hidden_size\n",
        "    \"hidden_dropout_prob\": 0.0,\n",
        "    \"attention_probs_dropout_prob\": 0.0,\n",
        "    \"initializer_range\": 0.02,\n",
        "    \"image_size\": 32,\n",
        "    \"num_classes\": 8, # num_classes\n",
        "    \"num_channels\": 3,\n",
        "    \"qkv_bias\": True,\n",
        "    \"use_faster_attention\": True,\n",
        "}\n",
        "# These are not hard constraints, but are used to prevent misconfigurations\n",
        "assert config[\"hidden_size\"] % config[\"num_attention_heads\"] == 0\n",
        "assert config['intermediate_size'] == 4 * config['hidden_size']\n",
        "assert config['image_size'] % config['patch_size'] == 0\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"\n",
        "    The simple trainer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, optimizer, loss_fn, exp_name, device):\n",
        "        self.model = model.to(device)\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.exp_name = exp_name\n",
        "        self.device = device\n",
        "\n",
        "    def train(self, trainloader, testloader, epochs, save_model_every_n_epochs=0):\n",
        "        \"\"\"\n",
        "        Train the model for the specified number of epochs.\n",
        "        \"\"\"\n",
        "        # Keep track of the losses and accuracies\n",
        "        train_losses, test_losses, accuracies = [], [], []\n",
        "        # Train the model\n",
        "        for i in range(epochs):\n",
        "            train_loss = self.train_epoch(trainloader)\n",
        "            accuracy, test_loss = self.evaluate(testloader)\n",
        "            train_losses.append(train_loss)\n",
        "            test_losses.append(test_loss)\n",
        "            accuracies.append(accuracy)\n",
        "            print(f\"Epoch: {i+1}, Train loss: {train_loss:.4f}, Test loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "            if save_model_every_n_epochs > 0 and (i+1) % save_model_every_n_epochs == 0 and i+1 != epochs:\n",
        "                print('\\tSave checkpoint at epoch', i+1)\n",
        "                save_checkpoint(self.exp_name, self.model, i+1)\n",
        "        # Save the experiment\n",
        "        save_experiment(self.exp_name, config, self.model, train_losses, test_losses, accuracies)\n",
        "\n",
        "    def train_epoch(self, trainloader):\n",
        "        \"\"\"\n",
        "        Train the model for one epoch.\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        for batch in trainloader:\n",
        "            # Move the batch to the device\n",
        "            batch = [t.to(self.device) for t in batch]\n",
        "            images, labels = batch\n",
        "            # Zero the gradients\n",
        "            self.optimizer.zero_grad()\n",
        "            # Calculate the loss\n",
        "            loss = self.loss_fn(self.model(images)[0], labels)\n",
        "            # Backpropagate the loss\n",
        "            loss.backward()\n",
        "            # Update the model's parameters\n",
        "            self.optimizer.step()\n",
        "            total_loss += loss.item() * len(images)\n",
        "        return total_loss / len(trainloader.dataset)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self, testloader):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in testloader:\n",
        "                # Move the batch to the device\n",
        "                batch = [t.to(self.device) for t in batch]\n",
        "                images, labels = batch\n",
        "\n",
        "                # Get predictions\n",
        "                logits, _ = self.model(images)\n",
        "\n",
        "                # Calculate the loss\n",
        "                loss = self.loss_fn(logits, labels)\n",
        "                total_loss += loss.item() * len(images)\n",
        "\n",
        "                # Calculate the accuracy\n",
        "                predictions = torch.argmax(logits, dim=1)\n",
        "                correct += torch.sum(predictions == labels).item()\n",
        "        accuracy = correct / len(testloader.dataset)\n",
        "        avg_loss = total_loss / len(testloader.dataset)\n",
        "        return accuracy, avg_loss"
      ],
      "metadata": {
        "id": "XCwAaOf3LcUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_args():\n",
        "    import argparse\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--exp-name\", type=str, required=True)\n",
        "    parser.add_argument(\"--batch-size\", type=int, default=256)\n",
        "    parser.add_argument(\"--epochs\", type=int, default=100)\n",
        "    parser.add_argument(\"--lr\", type=float, default=1e-2)\n",
        "    parser.add_argument(\"--device\", type=str)\n",
        "    parser.add_argument(\"--save-model-every\", type=int, default=0)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    if args.device is None:\n",
        "        args.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    return args\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "    # Training parameters\n",
        "    batch_size = args.batch_size\n",
        "    epochs = args.epochs\n",
        "    lr = args.lr\n",
        "    device = args.device\n",
        "    save_model_every_n_epochs = args.save_model_every\n",
        "    # Load the CIFAR10 dataset\n",
        "    trainloader, testloader, _ = prepare_data(batch_size=batch_size)\n",
        "    # Create the model, optimizer, loss function and trainer\n",
        "    model = ViTForClassfication(config)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    trainer = Trainer(model, optimizer, loss_fn, args.exp_name, device=device)\n",
        "    trainer.train(trainloader, testloader, epochs, save_model_every_n_epochs=save_model_every_n_epochs)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "h7pyF91GMHLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YOLO"
      ],
      "metadata": {
        "id": "eQuAx77cyR7j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## setup"
      ],
      "metadata": {
        "id": "OugEcD33Bdgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(f'Setup complete. Using torch {torch.__version__}')\n",
        "print(f\"{torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'}\")"
      ],
      "metadata": {
        "id": "uzBPEltSyO1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ],
      "metadata": {
        "id": "TjzD3oKn4pmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ultralytics\n",
        "\n",
        "ultralytics.checks()"
      ],
      "metadata": {
        "id": "OGjqYCiULRMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "bUhWJorj4_c_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir dataset/ohm_final_dance\n",
        "!unzip dataset/split_dataset.zip -d dataset/ohm_final_dance"
      ],
      "metadata": {
        "id": "sa_K2NSOcg50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir 400223-Art/dataset/super_contrast\n",
        "!unzip super_contrast.zip -d 400223-Art/dataset/super_contrast"
      ],
      "metadata": {
        "id": "sdedGoql-0xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir 400223-Art/dataset/super_contrast_test\n",
        "!unzip super_contrast_test.zip -d 400223-Art/dataset/super_contrast_test"
      ],
      "metadata": {
        "id": "8nXs67wW-_cA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "UVwh5I_2BhvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('/lustrefs/disk/project/lt900103-ai24e3/400223-Art/yolov8l.pt')\n",
        "params = {\n",
        "    \"task\": \"detect\",\n",
        "    \"mode\": \"train\",\n",
        "    \"model\": \"yolov8x.pt\",\n",
        "    \"data\": \"liver.yaml\",\n",
        "    \"epochs\": 100,\n",
        "    \"batch\": 32,\n",
        "    \"agnostic_nms\": True,\n",
        "    \"cos_lr\": True,\n",
        "    \"device\": [0,1]\n",
        "}\n",
        "\n",
        "model.train(**params)"
      ],
      "metadata": {
        "id": "i-bXsLFVIlRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo task=detect mode=train epochs=100 batch=64 plots=True model=yolov8l.pt data=liver.yaml device=[0,1,2,3] imgsz=512"
      ],
      "metadata": {
        "id": "OkHxO_xm5Ucm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict"
      ],
      "metadata": {
        "id": "h85XLg4YBl6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "sp_sb = pd.read_csv('dataset/sample_submission.csv')\n",
        "sp_sb"
      ],
      "metadata": {
        "id": "aLdw5KfM1doZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_objects(image_path, model):\n",
        "    results = model.predict(source=image_path, conf=0.600125)\n",
        "    boxes = results[0].boxes.xyxy[:, :4].cpu().numpy()\n",
        "    classes = results[0].boxes.cls.cpu().numpy()\n",
        "    scores = results[0].boxes.conf.cpu().numpy()\n",
        "    boxes_n = []\n",
        "    for boxe in boxes:\n",
        "      boxe = [int(i) for i in boxe]\n",
        "      boxes_n.append(boxe)\n",
        "    classes = [int(classe) for classe in classes]\n",
        "    scores = [round(score, 4) for score in scores]\n",
        "    return boxes_n, classes, scores"
      ],
      "metadata": {
        "id": "wwbqpxzQBQZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "inference_model = YOLO('runs/detect/train6/weights/best.pt')\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "DcGASkip10Fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "annotation_ls = []\n",
        "label_ls = []\n",
        "\n",
        "for name in tqdm(sp_sb['Image File']):\n",
        "  image_path = 'dataset/ohm_final_dance/test/images/{name_img}.jpg'.format(name_img=name)\n",
        "  boxes, classes, scores = detect_objects(image_path, inference_model)\n",
        "  annotation_ls.append(boxes)\n",
        "  label_ls.append(classes)"
      ],
      "metadata": {
        "id": "bE-Ro5Rd1_-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "annotation_ls[0:3] = sp_sb['Annotation'][0:3]\n",
        "annotation_ls[24] = sp_sb['Annotation'][24]\n",
        "annotation_ls[115] = sp_sb['Annotation'][115]\n",
        "label_ls[0:3] = sp_sb['Label'][0:3]\n",
        "label_ls[24] = sp_sb['Label'][24]\n",
        "label_ls[115] = sp_sb['Label'][115]"
      ],
      "metadata": {
        "id": "xI785nfoAKd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp_sb['Annotation'] = annotation_ls\n",
        "sp_sb['Label'] = label_ls\n",
        "sp_sb"
      ],
      "metadata": {
        "id": "9A7TYTSk_kGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp_sb.to_csv('boss_c600125.csv', index=False)"
      ],
      "metadata": {
        "id": "cyzJBdSR5SFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- compare"
      ],
      "metadata": {
        "id": "ZeNmsP2xF8N0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sp_sb = pd.read_csv('pred_sub/boss_c58.csv')"
      ],
      "metadata": {
        "id": "fDXz2WhLGqJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_art = pd.read_csv('pred_sub/boss_c58.csv')\n",
        "best_art"
      ],
      "metadata": {
        "id": "X0CmPxnxGCCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp_sb[sp_sb['Annotation'] == best_art['Annotation']]"
      ],
      "metadata": {
        "id": "aQcvu6X2F2OY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp_sb[sp_sb['Label'] == best_art['Label']]"
      ],
      "metadata": {
        "id": "HrskIpxmG7q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhox2BAs5REh"
      },
      "outputs": [],
      "source": []
    }
  ]
}