{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Lhan2QxdAp1U",
        "gwbHWrK2Avto",
        "xR2Epr1dj6qx",
        "LYu83fBjFsss",
        "xVdCIH_Cegx5",
        "SfvcIPoPfhvK",
        "vN_SZ7huelmX",
        "OWJ_M44ptknq",
        "HTpdX2AbeLbY",
        "INsBKxxNccpC",
        "n15Ex_KYE3Vm",
        "5YkHRIoQ__PQ",
        "DSuUsSO3AOrJ",
        "hDHoR7HuCku6",
        "dV1XO8WfCnL4",
        "1c0cK3xNCpBa",
        "av-MLHEIDpUp"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebPvs_UpIWtV"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "username = userdata.get('KAGGLE_USER')\n",
        "key = userdata.get('KAGGLE_KEY')\n",
        "# Echo the credentials into the kaggle.json file\n",
        "!mkdir -p ~/.kaggle\n",
        "!echo '{{\"username\":\"{username}\",\"key\":\"{key}\"}}' > ~/.kaggle/kaggle.json\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset\n",
        "!kaggle competitions download -c liver-ultrasound-detection"
      ],
      "metadata": {
        "id": "4LrGz0P2M-x8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/liver-ultrasound-detection.zip && rm -rf /content/liver-ultrasound-detection.zip"
      ],
      "metadata": {
        "id": "qPZpzjmYXF4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "BsAXmAfiamz7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read img"
      ],
      "metadata": {
        "id": "IaaEkKKGcP5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ej7wc3vZalxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the image: machine negative\n",
        "image_tag = 19793\n",
        "image_path = f'/content/train/train/images/{image_tag}.jpg'\n",
        "image = cv2.imread(image_path, cv2.COLOR_BGR2RGB)\n",
        "if type(image) == type(None):\n",
        "  image_path = f'/content/val/val/images/{image_tag}.jpg'\n",
        "  image = cv2.imread(image_path, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "height, width = image.shape[0], image.shape[1]\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ByIp1KWPBD_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the image: machine positive\n",
        "image_tag = 93078\n",
        "image_path = f'/content/train/train/images/{image_tag}.jpg'\n",
        "image = cv2.imread(image_path, cv2.COLOR_BGR2RGB)\n",
        "if type(image) == type(None):\n",
        "  image_path = f'/content/val/val/images/{image_tag}.jpg'\n",
        "  image = cv2.imread(image_path, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "height, width = image.shape[0], image.shape[1]\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WatvrDf_BjRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the image: mobile negative\n",
        "image_tag = 3544\n",
        "image_path = f'/content/train/train/images/{image_tag}.jpg'\n",
        "image = cv2.imread(image_path, cv2.COLOR_BGR2RGB)\n",
        "if type(image) == type(None):\n",
        "  image_path = f'/content/val/val/images/{image_tag}.jpg'\n",
        "  image = cv2.imread(image_path, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "height, width = image.shape[0], image.shape[1]\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vV7O7KW6AItI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the image: mobile positive\n",
        "image_tag = 128189\n",
        "image_path = f'/content/train/train/images/{image_tag}.jpg'\n",
        "image = cv2.imread(image_path, cv2.COLOR_BGR2RGB)\n",
        "if type(image) == type(None):\n",
        "  image_path = f'/content/val/val/images/{image_tag}.jpg'\n",
        "  image = cv2.imread(image_path, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "height, width = image.shape[0], image.shape[1]\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zfPeG8Uea7B7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## add bounding box"
      ],
      "metadata": {
        "id": "Lhan2QxdAp1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_boxes(image, tag):\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
        "  train_annotation_path = f'/content/train/train/annotations/{tag}.txt'\n",
        "  test_annotation_path = f'/content/val/val/annotations/{tag}.txt'\n",
        "\n",
        "  try:\n",
        "    # Read the YOLO annotation file\n",
        "    with open(train_annotation_path, 'r') as file:\n",
        "      lines = file.readlines()\n",
        "  except:\n",
        "\n",
        "    with open(test_annotation_path, 'r') as file:\n",
        "      lines = file.readlines()\n",
        "\n",
        "  for line in lines:\n",
        "    print(line)\n",
        "    # Split the line into components\n",
        "    components = line.strip().split()\n",
        "    class_id = int(components[0])\n",
        "    x_center = float(components[1])\n",
        "    y_center = float(components[2])\n",
        "    bbox_width = float(components[3])\n",
        "    bbox_height = float(components[4])\n",
        "\n",
        "    # Convert normalized coordinates to pixel coordinates\n",
        "    x_center_pixel = int(x_center * width)\n",
        "    y_center_pixel = int(y_center * height)\n",
        "    bbox_width_pixel = int(bbox_width * width)\n",
        "    bbox_height_pixel = int(bbox_height * height)\n",
        "\n",
        "    # Calculate the top-left and bottom-right corners of the bounding box\n",
        "    top_left = (x_center_pixel - bbox_width_pixel // 2, y_center_pixel - bbox_height_pixel // 2)\n",
        "    bottom_right = (x_center_pixel + bbox_width_pixel // 2, y_center_pixel + bbox_height_pixel // 2)\n",
        "\n",
        "    # Draw the rectangle on the image\n",
        "    color = (0, 255, 0)  # Green color for bounding box\n",
        "    thickness = 3\n",
        "    cv2.rectangle(image, top_left, bottom_right, color, thickness)\n",
        "\n",
        "    text_color = (255, 255, 255)\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = min(width, height) / 1000.0\n",
        "    font_thickness = max(1, int(font_scale * 2))\n",
        "    text = str(class_id)\n",
        "    text_size, _ = cv2.getTextSize(text, font, font_scale, font_thickness)\n",
        "    text_origin = (top_left[0]+ (bbox_width_pixel//2)-10, top_left[1] - 5)\n",
        "    # Ensure text is within image bounds\n",
        "    text_origin = (max(text_origin[0], 0), max(text_origin[1], text_size[1]))\n",
        "    cv2.putText(image, text, text_origin, font, font_scale, text_color, font_thickness)\n",
        "\n",
        "    # Convert the image from BGR to RGB format for displaying with Matplotlib\n",
        "  image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  return image_rgb"
      ],
      "metadata": {
        "id": "cop9ncB7ZwYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## try to infer"
      ],
      "metadata": {
        "id": "gwbHWrK2Avto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(add_boxes(cv2.cvtColor(image, cv2.COLOR_RGB2GRAY), image_tag), cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "txo1uTAb-kvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(add_boxes(cv2.cvtColor(image, cv2.COLOR_RGB2GRAY), image_tag), cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z6QOSZqBChU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Enhancement"
      ],
      "metadata": {
        "id": "eoEB6X5hXXiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "plt.imshow(gray_image, cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SZrbhAnTzSSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Invert black to white"
      ],
      "metadata": {
        "id": "xR2Epr1dj6qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def invert_lib(image_file, with_plot=True, gray_scale=True):\n",
        "    cmap_val = None if not gray_scale else 'gray'\n",
        "\n",
        "    image_i = cv2.bitwise_not(image_file)  # image_i = 255 - image_src\n",
        "\n",
        "    if with_plot:\n",
        "        fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10, 20))\n",
        "\n",
        "        ax1.axis(\"off\")\n",
        "        ax1.title.set_text('Original')\n",
        "\n",
        "        ax2.axis(\"off\")\n",
        "        ax2.title.set_text(\"Inverted\")\n",
        "\n",
        "        ax1.imshow(image_file, cmap=cmap_val)\n",
        "        ax2.imshow(image_i, cmap=cmap_val)\n",
        "        return True\n",
        "    return image_i\n",
        "\n",
        "invert_lib(gray_image)"
      ],
      "metadata": {
        "id": "foRel23YkAtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inv_img = invert_lib(gray_image, with_plot=False)\n",
        "inv_img.shape"
      ],
      "metadata": {
        "id": "bqWrxEr4l-xR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- white to black(RGB)"
      ],
      "metadata": {
        "id": "BE7oJR5coUYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(cv2.bitwise_and(image, image, inv_img), cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ilZuO-R7m0xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv2.bitwise_and(image, image, inv_img).shape"
      ],
      "metadata": {
        "id": "exHen9VIoMxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intensity Transformation"
      ],
      "metadata": {
        "id": "LYu83fBjFsss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### auto scaling"
      ],
      "metadata": {
        "id": "xVdCIH_Cegx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def auto_scale_intensity(image):\n",
        "    # Convert the image to float32 type for precision\n",
        "    image_float = image.astype(np.float32)\n",
        "\n",
        "    # Compute the minimum and maximum pixel values\n",
        "    min_val = np.min(image_float)\n",
        "    max_val = np.max(image_float)\n",
        "\n",
        "    # Scale the intensity to the range [0, 255]\n",
        "    scaled_image = 255 * (image_float - min_val) / (max_val - min_val)\n",
        "\n",
        "    # Convert back to uint8 type\n",
        "    scaled_image = scaled_image.astype(np.uint8)\n",
        "\n",
        "    return scaled_image\n",
        "\n",
        "# Apply auto-scaling\n",
        "intenT_img = auto_scale_intensity(gray_image)\n",
        "plt.imshow(intenT_img, cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OsO1-fToADY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intenT_img"
      ],
      "metadata": {
        "id": "kWGo6uDcv4oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### contrast Stretching"
      ],
      "metadata": {
        "id": "SfvcIPoPfhvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def contrast_stretching(image):\n",
        "    # Compute the minimum and maximum pixel values\n",
        "    min_val = np.min(image)\n",
        "    max_val = np.max(image)\n",
        "\n",
        "    # Apply contrast stretching formula\n",
        "    stretched_image = 255 * (image - min_val) / (max_val - min_val)\n",
        "\n",
        "    # Convert back to uint8 type\n",
        "    stretched_image = np.uint8(stretched_image)\n",
        "\n",
        "    return stretched_image\n",
        "\n",
        "intenT_img = contrast_stretching(gray_image)\n",
        "\n",
        "plt.imshow(intenT_img, cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zxfd6kSUfhUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intenT_img"
      ],
      "metadata": {
        "id": "c1Elid9HrBkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intenT_img = contrast_stretching(inv_img)\n",
        "\n",
        "plt.imshow(intenT_img, cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UpaD_i34qlPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intenT_img"
      ],
      "metadata": {
        "id": "DgkoIoCPq_8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BIMEF"
      ],
      "metadata": {
        "id": "vN_SZ7huelmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MSR(grayscale_image, sigma_list):\n",
        "    \"\"\"\n",
        "    Multi-Scale Retinex (MSR) for grayscale images.\n",
        "    \"\"\"\n",
        "    # Convert the image to float32\n",
        "    grayscale_image = grayscale_image.astype(np.float32) / 255.0\n",
        "\n",
        "    # Logarithmic transformation\n",
        "    log_image = np.log1p(grayscale_image)\n",
        "\n",
        "    # Apply Gaussian blurs with different sigmas\n",
        "    blurred_images = [cv2.GaussianBlur(log_image, (0, 0), sigma) for sigma in sigma_list]\n",
        "\n",
        "    # Retinex (subtract the blurred image from the original)\n",
        "    retinex = np.zeros_like(grayscale_image)\n",
        "    for blurred in blurred_images:\n",
        "        retinex += log_image - np.log1p(blurred)\n",
        "\n",
        "    retinex /= len(sigma_list)\n",
        "\n",
        "    # Normalize the result to [0, 255]\n",
        "    msr = (retinex - np.min(retinex)) / (np.max(retinex) - np.min(retinex)) * 255\n",
        "    msr = np.uint8(msr)\n",
        "\n",
        "    return msr\n",
        "\n",
        "# Apply MSRCR with different sigma values for multi-scale retinex\n",
        "sigma_list = [15, 80, 250]\n",
        "intenT_img = MSR(gray_image, sigma_list)\n",
        "\n",
        "plt.imshow(intenT_img, cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZStkdw6tgmu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intenT_img"
      ],
      "metadata": {
        "id": "YmNRljMrilGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# black -> white\n",
        "\n",
        "intenT_img = MSR(inv_img, sigma_list)\n",
        "\n",
        "plt.imshow(intenT_img, cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sE3yuL9smWgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intenT_img"
      ],
      "metadata": {
        "id": "UBFeGZE7mg5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### gamma Correction"
      ],
      "metadata": {
        "id": "OWJ_M44ptknq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gamma_correction(image, gamma):\n",
        "    # Build a lookup table mapping pixel values [0, 255] to their adjusted gamma values\n",
        "    invGamma = 1.0 / gamma\n",
        "    table = np.array([(i / 255.0) ** invGamma * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
        "\n",
        "    # Apply gamma correction using the lookup table\n",
        "    corrected_image = cv2.LUT(image, table)\n",
        "\n",
        "    return corrected_image"
      ],
      "metadata": {
        "id": "DjWEO4tLtj86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the gamma value\n",
        "gamma_value = 1  # You can adjust this value as needed\n",
        "\n",
        "# Apply gamma correction\n",
        "intenT_img = gamma_correction(gray_image, gamma_value)\n",
        "\n",
        "plt.imshow(intenT_img, cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P-jH8U9aukAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the gamma value\n",
        "gamma_value = 1  # You can adjust this value as needed\n",
        "\n",
        "# Apply gamma correction\n",
        "intenT_img = gamma_correction(inv_img, gamma_value)\n",
        "\n",
        "plt.imshow(intenT_img, cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_PTJSlyLq9rB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the gamma value\n",
        "gamma_value = 3  # You can adjust this value as needed\n",
        "\n",
        "# Apply gamma correction\n",
        "intenT_img = gamma_correction(gray_image, gamma_value)\n",
        "\n",
        "plt.imshow(intenT_img, cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g__J05dq1Zek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the gamma value\n",
        "gamma_value = 3  # You can adjust this value as needed\n",
        "\n",
        "# Apply gamma correction\n",
        "intenT_img = gamma_correction(inv_img, gamma_value)\n",
        "\n",
        "plt.imshow(intenT_img, cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fOSdcuaM1nwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the gamma value\n",
        "gamma_value = -1  # You can adjust this value as needed\n",
        "\n",
        "# Apply gamma correction\n",
        "intenT_img = gamma_correction(gray_image, gamma_value)\n",
        "\n",
        "plt.imshow(add_boxes(intenT_img, image_tag), cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ysNTqg7v6ytL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the gamma value\n",
        "gamma_value = -1  # You can adjust this value as needed\n",
        "\n",
        "# Apply gamma correction\n",
        "intenT_img = gamma_correction(inv_img, gamma_value)\n",
        "\n",
        "plt.imshow(add_boxes(intenT_img, image_tag), cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-Z4jhWha1uK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gamma_value = -3  # You can adjust this value as needed\n",
        "\n",
        "# Apply gamma correction\n",
        "intenT_img = gamma_correction(gray_image, gamma_value)\n",
        "\n",
        "plt.imshow(add_boxes(intenT_img, image_tag), cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dSHZayMy23Rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the gamma value\n",
        "gamma_value = -3  # You can adjust this value as needed\n",
        "\n",
        "# Apply gamma correction\n",
        "intenT_img = gamma_correction(inv_img, gamma_value)\n",
        "\n",
        "plt.imshow(add_boxes(intenT_img, image_tag), cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uQcALnwH6PT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### log Transform"
      ],
      "metadata": {
        "id": "JIX2I92qCzmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_transform(image):\n",
        "    # Apply logarithmic transformation to the image\n",
        "    log_image = np.log1p(image.astype(np.float32))\n",
        "\n",
        "    # Normalize the transformed image to [0, 255]\n",
        "    log_image = (log_image - np.min(log_image)) / (np.max(log_image) - np.min(log_image)) * 255\n",
        "\n",
        "    # Convert back to uint8 type\n",
        "    log_image = np.uint8(log_image)\n",
        "\n",
        "    return log_image"
      ],
      "metadata": {
        "id": "9w3Q4cXKCqv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intenT_img = log_transform(gray_image)\n",
        "\n",
        "plt.imshow(add_boxes(intenT_img, image_tag), cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A4jKR2oSDXWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intenT_img = log_transform(inv_img)\n",
        "\n",
        "plt.imshow(add_boxes(intenT_img, image_tag), cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VR0wDxNlDaoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intenT_img = log_transform(gray_image)\n",
        "\n",
        "plt.imshow(add_boxes(intenT_img, image_tag), cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uFcr00rWC_qN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intenT_img = log_transform(inv_img)\n",
        "\n",
        "plt.imshow(add_boxes(intenT_img, image_tag), cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-j4728F1C3Y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5VIEyrsMCql0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CEUS (Fake by GPT)"
      ],
      "metadata": {
        "id": "gl5RM1R69K1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_ceus(image, enhancement_factor=2):\n",
        "    # Convert image to float32 for processing\n",
        "    image_float = image.astype(np.float32)\n",
        "\n",
        "    # Apply enhancement by boosting pixel values\n",
        "    enhanced_image = image_float * enhancement_factor\n",
        "\n",
        "    # Normalize the enhanced image to [0, 255] range\n",
        "    enhanced_image = np.clip(enhanced_image, 0, 255).astype(np.uint8)\n",
        "\n",
        "    return enhanced_image\n",
        "\n",
        "intenT_img = simulate_ceus(gray_image)\n",
        "\n",
        "plt.imshow(add_boxes(intenT_img, image_tag), cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CZ-a1AxT9J78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intenT_img = simulate_ceus(inv_img)\n",
        "\n",
        "plt.imshow(add_boxes(intenT_img, image_tag), cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KUSaCnKLeYbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intenT_img = simulate_ceus(gray_image)\n",
        "\n",
        "plt.imshow(add_boxes(intenT_img, image_tag), cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D9wvOKWXBamL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intenT_img = simulate_ceus(inv_img)\n",
        "\n",
        "plt.imshow(add_boxes(intenT_img, image_tag), cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Enr2F0YKBaiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Histogram Processing"
      ],
      "metadata": {
        "id": "ThaT_1kMeBeC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Histogram Equalization"
      ],
      "metadata": {
        "id": "HTpdX2AbeLbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(cv2.equalizeHist(gray_image), cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s9X-e7EXdloJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CLAHE"
      ],
      "metadata": {
        "id": "INsBKxxNccpC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- machine positive"
      ],
      "metadata": {
        "id": "aQLAb038DDEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Create a CLAHE object\n",
        "clahe = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(8, 8))\n",
        "\n",
        "# Step 3: Apply CLAHE to the grayscale image\n",
        "clahe_image = clahe.apply(gray_image)\n",
        "plt.imshow(clahe_image, cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9RUi9v4uDQLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- mobile positive"
      ],
      "metadata": {
        "id": "zkNy1i76DFXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Create a CLAHE object\n",
        "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "\n",
        "# Step 3: Apply CLAHE to the grayscale image\n",
        "clahe_image = clahe.apply(gray_image)\n",
        "plt.imshow(clahe_image, cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9d44uUPdcW5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clahe_image, clahe_image.shape"
      ],
      "metadata": {
        "id": "HyA3G4aa-HEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spatial Filter"
      ],
      "metadata": {
        "id": "n15Ex_KYE3Vm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Smoothing (Blurring)"
      ],
      "metadata": {
        "id": "5YkHRIoQ__PQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Averaging"
      ],
      "metadata": {
        "id": "DSuUsSO3AOrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blur = cv2.blur(gray_image, (5,5))   # kernel of 5x5 size\n",
        "\n",
        "plt.subplot(121),plt.imshow(gray_image, cmap='gray'),plt.title('Original')\n",
        "plt.xticks([]), plt.yticks([])\n",
        "plt.subplot(122),plt.imshow(blur, cmap='gray'),plt.title('Blurred')\n",
        "plt.xticks([]), plt.yticks([])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nza_DOyC_4Wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gaussian Blurring"
      ],
      "metadata": {
        "id": "hDHoR7HuCku6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blur = cv2.blur(gray_image, (5,5), 0.2)   # kernel of 5x5 size\n",
        "\n",
        "plt.subplot(121),plt.imshow(gray_image, cmap='gray'),plt.title('Original')\n",
        "plt.xticks([]), plt.yticks([])\n",
        "plt.subplot(122),plt.imshow(blur, cmap='gray'),plt.title('Blurred')\n",
        "plt.xticks([]), plt.yticks([])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eR2TGwZqCklW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Median Blurring"
      ],
      "metadata": {
        "id": "dV1XO8WfCnL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blur = cv2.medianBlur(gray_image, 5, 0.2)   # kernel of 5x5 size\n",
        "\n",
        "plt.subplot(121),plt.imshow(gray_image, cmap='gray'),plt.title('Original')\n",
        "plt.xticks([]), plt.yticks([])\n",
        "plt.subplot(122),plt.imshow(blur, cmap='gray'),plt.title('Blurred')\n",
        "plt.xticks([]), plt.yticks([])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wOCqBeo-CkiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bilateral Filtering"
      ],
      "metadata": {
        "id": "1c0cK3xNCpBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blur = cv2.bilateralFilter(gray_image,9,75,75)   # kernel of 5x5 size\n",
        "\n",
        "plt.subplot(121),plt.imshow(gray_image, cmap='gray'),plt.title('Original')\n",
        "plt.xticks([]), plt.yticks([])\n",
        "plt.subplot(122),plt.imshow(blur, cmap='gray'),plt.title('Blurred')\n",
        "plt.xticks([]), plt.yticks([])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rBZFlQ-nCkfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sharpening"
      ],
      "metadata": {
        "id": "av-MLHEIDpUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(cv2.Laplacian(gray_image,cv2.CV_64F), cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WbsCb0-zchaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cavumorJD8DJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# P' Pim filter & dataset"
      ],
      "metadata": {
        "id": "tRxcI3jf3ybw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_ultrasound(image, imtype='mobile'):\n",
        "  if imtype == 'machine' or imtype == 'mobile':\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    print(np.mean(gray))\n",
        "    ret,th1 = cv2.threshold(gray, np.mean(gray), 255 ,cv2.THRESH_BINARY)\n",
        "    # ret,th1 = cv2.threshold(gray, np.mean(gray),255 ,cv2.THRESH_BINARY)\n",
        "    im_floodfill = th1.copy()\n",
        "    h, w = im_floodfill.shape[:2]\n",
        "    mask_floodfill = np.zeros((h+2, w+2), np.uint8)\n",
        "    cv2.floodFill(im_floodfill, mask_floodfill, (0,0), 255)\n",
        "    filled_mask = cv2.bitwise_or(th1, cv2.bitwise_not(im_floodfill))\n",
        "\n",
        "    # Smooth the left edge\n",
        "    contours, _ = cv2.findContours(filled_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    for contour in contours:\n",
        "        if cv2.contourArea(contour) > 500:  # Only consider significant contours\n",
        "            epsilon = 0.01 * cv2.arcLength(contour, True)\n",
        "            approx = cv2.approxPolyDP(contour, epsilon, True)\n",
        "            hull = cv2.convexHull(approx)\n",
        "            cv2.drawContours(filled_mask, [hull], 0, (255), thickness=cv2.FILLED)\n",
        "\n",
        "    flipped_mask = cv2.flip(filled_mask, 1)\n",
        "    sum_mask = filled_mask + flipped_mask\n",
        "    im_floodfill = sum_mask.copy()\n",
        "    h, w = im_floodfill.shape[:2]\n",
        "    mask_floodfill = np.zeros((h+2, w+2), np.uint8)\n",
        "    cv2.floodFill(im_floodfill, mask_floodfill, (0,0), 255)\n",
        "    filled_mask = cv2.bitwise_or(sum_mask, cv2.bitwise_not(im_floodfill))\n",
        "    cleaned_mask = cv2.morphologyEx(filled_mask, cv2.MORPH_OPEN, np.ones((5, 5), np.uint8))\n",
        "    contours, _ = cv2.findContours(cleaned_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    hull_mask = np.zeros_like(gray, dtype=np.uint8)\n",
        "    if contours:\n",
        "        largest_contour = max(contours, key=cv2.contourArea)\n",
        "        hull = cv2.convexHull(largest_contour)\n",
        "        cv2.drawContours(hull_mask, [hull], -1, 255, thickness=cv2.FILLED)\n",
        "    masked_image = cv2.bitwise_and(image, image, mask=hull_mask)\n",
        "\n",
        "  else: #Gital's generated US\n",
        "\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    adapthresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C,\n",
        "                                              cv2.THRESH_BINARY, 199, 2)\n",
        "    ret, mask = cv2.threshold(adapthresh, 0, 255, cv2.THRESH_BINARY )# for generated mobile-like\n",
        "    kernel = np.ones((5, 5), np.uint8)\n",
        "    opened_mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
        "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(opened_mask, connectivity=8)\n",
        "    largest_component = 1 + np.argmax(stats[1:, cv2.CC_STAT_AREA])\n",
        "    largest_mask = np.zeros_like(opened_mask)\n",
        "    largest_mask[labels == largest_component] = 255\n",
        "    im_floodfill = largest_mask.copy()\n",
        "    h, w = im_floodfill.shape[:2]\n",
        "    mask_floodfill = np.zeros((h+2, w+2), np.uint8)\n",
        "    cv2.floodFill(im_floodfill, mask_floodfill, (0,0), 255)\n",
        "    filled_mask = cv2.bitwise_or(largest_mask, cv2.bitwise_not(im_floodfill))\n",
        "    contours, _ = cv2.findContours(filled_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    for contour in contours:\n",
        "        if cv2.contourArea(contour) > 500:  # Only consider significant contours\n",
        "            epsilon = 0.01 * cv2.arcLength(contour, True)\n",
        "            approx = cv2.approxPolyDP(contour, epsilon, True)\n",
        "            hull = cv2.convexHull(approx)\n",
        "            cv2.drawContours(filled_mask, [hull], 0, (255), thickness=cv2.FILLED)\n",
        "    masked_image = cv2.bitwise_and(image, image, mask=filled_mask)\n",
        "\n",
        "  return masked_image"
      ],
      "metadata": {
        "id": "BV0hVDzz33LL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_image(image, title, cmap='gray'):\n",
        "    \"\"\"Displays an image using matplotlib.\"\"\"\n",
        "    plt.imshow(image, cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "v-554rDV4xIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def append_files_in_directory(directory_path):\n",
        "    # List all items in the directory\n",
        "    all_items = os.listdir(directory_path)\n",
        "    # Filter out only files (not directories)\n",
        "    files = [os.path.join(directory_path, item) for item in all_items if os.path.isfile(os.path.join(directory_path, item))]\n",
        "    return files\n",
        "\n",
        "def check_twin(img_path):\n",
        "  # Get dimensions\n",
        "  image = cv2.imread(img_path)\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "  height, width = image.shape\n",
        "\n",
        "  # Calculate the dimensions for the crop\n",
        "  crop_width = width // 3\n",
        "  crop_height = height // 2\n",
        "  left = (width - crop_width) // 2\n",
        "  top = 0  # Start from the top\n",
        "\n",
        "  # Define the crop box\n",
        "  right = left + crop_width\n",
        "  bottom = top + crop_height\n",
        "\n",
        "  # Crop the image\n",
        "  cropped_image = image[top:bottom, left:right]\n",
        "\n",
        "  # return\n",
        "  if np.mean(cropped_image) >= np.mean(image):\n",
        "    return \"once\"\n",
        "  else:\n",
        "    return \"twice\"\n",
        "\n",
        "train_ls = append_files_in_directory('/content/train/train/images')\n",
        "val_ls = append_files_in_directory('/content/val/val/images')\n",
        "test_ls = append_files_in_directory('/content/test/test/images')\n",
        "\n",
        "twin_train = pd.Series(train_ls).progress_apply(check_twin)\n",
        "twin_val = pd.Series(val_ls).progress_apply(check_twin)\n",
        "twin_test = pd.Series(test_ls).progress_apply(check_twin)"
      ],
      "metadata": {
        "id": "BejH53oYn6v3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "twin_train.value_counts()"
      ],
      "metadata": {
        "id": "HJcc8GpQymMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "twin_val.value_counts()"
      ],
      "metadata": {
        "id": "GMgGqrx3y3Bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "twin_test.value_counts()"
      ],
      "metadata": {
        "id": "MzqwuhHhywAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tw_df_train = pd.DataFrame(data={\"file\": train_ls, 'twin': twin_train})\n",
        "tw_df_val = pd.DataFrame(data={\"file\": val_ls, 'twin': twin_val})\n",
        "tw_df_test = pd.DataFrame(data={\"file\": test_ls, 'twin': twin_test})"
      ],
      "metadata": {
        "id": "uIlANzii4pHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tw_df_train = tw_df_train[tw_df_train['twin'] == \"twice\"]\n",
        "tw_df_val = tw_df_val[tw_df_val['twin'] == \"twice\"]\n",
        "tw_df_test = tw_df_test[tw_df_test['twin'] == \"twice\"]"
      ],
      "metadata": {
        "id": "4v3-2GbH6hXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tw = pd.concat([tw_df_train, tw_df_val, tw_df_test])\n",
        "df_tw"
      ],
      "metadata": {
        "id": "FrbGEco_7GrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tw.to_csv('twin_liver.csv', index=False)"
      ],
      "metadata": {
        "id": "C5Gma1K278TG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P4oTPMun7GkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = '/content/train/train/images/1713.jpg'\n",
        "\n",
        "image = cv2.imread(image_path)\n",
        "display_image(image, 'mobile train', cmap=None)\n",
        "\n",
        "test = extract_ultrasound(image, imtype='mobile')\n",
        "display_image(test, 'twin tail', cmap=None)"
      ],
      "metadata": {
        "id": "TY-QVEx2huj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get dimensions\n",
        "height, width, _ = image.shape\n",
        "\n",
        "# Calculate the dimensions for the crop\n",
        "crop_width = width // 3\n",
        "crop_height = height // 2\n",
        "left = (width - crop_width) // 2\n",
        "top = 0  # Start from the top\n",
        "\n",
        "# Define the crop box\n",
        "right = left + crop_width\n",
        "bottom = top + crop_height\n",
        "\n",
        "# Crop the image\n",
        "cropped_image = image[top:bottom, left:right]\n",
        "display_image(cropped_image, 'crob twin', cmap=None)\n",
        "np.mean(cropped_image)"
      ],
      "metadata": {
        "id": "ZykMIwtJmTAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = '/content/train/train/images/4483.jpg'\n",
        "\n",
        "image = cv2.imread(image_path)\n",
        "display_image(image, 'mobile train', cmap=None)\n",
        "\n",
        "test = extract_ultrasound(image, imtype='mobile')\n",
        "display_image(test, 'output', cmap=None)"
      ],
      "metadata": {
        "id": "iox1mXyTmZiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = '/content/val/val/images/100.jpg'\n",
        "\n",
        "image = cv2.imread(image_path)\n",
        "display_image(image, 'mobile val', cmap=None)\n",
        "\n",
        "test = extract_ultrasound(image, imtype='mobile')\n",
        "display_image(test, 'output', cmap=None)"
      ],
      "metadata": {
        "id": "ReeP686168Sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = '/content/test/test/images/10001.jpg'\n",
        "\n",
        "image = cv2.imread(image_path)\n",
        "display_image(image, 'machine test', cmap=None)\n",
        "\n",
        "test = extract_ultrasound(image, imtype='machine')\n",
        "display_image(test, 'output', cmap=None)"
      ],
      "metadata": {
        "id": "NOUT-ETT4Hal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = '/content/val/val/images/20584.jpg'\n",
        "\n",
        "image = cv2.imread(image_path)\n",
        "display_image(image, 'machine val')\n",
        "\n",
        "test = extract_ultrasound(image, imtype='machine')\n",
        "display_image(test, 'val', cmap=None)"
      ],
      "metadata": {
        "id": "dfsr23O6_c9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = '/content/train/train/images/103315.jpg'\n",
        "\n",
        "image = cv2.imread(image_path)\n",
        "display_image(image, 'machine train')\n",
        "\n",
        "test = extract_ultrasound(image, imtype='machine')\n",
        "display_image(test, 'val', cmap=None)"
      ],
      "metadata": {
        "id": "Dld1oqkkDdDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yuInOpCG4HDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Export Data"
      ],
      "metadata": {
        "id": "3XRAaXrL-ZmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "vN6z9oOCEM3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ganset512_t38k_v200\n",
        "!mkdir ganset512_t38k_v200/train\n",
        "!mkdir ganset512_t38k_v200/train/machine\n",
        "!mkdir ganset512_t38k_v200/train/mobile\n",
        "!mkdir ganset512_t38k_v200/val\n",
        "!mkdir ganset512_t38k_v200/val/machine\n",
        "!mkdir ganset512_t38k_v200/val/mobile"
      ],
      "metadata": {
        "id": "LpOU6-3mGPoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ganset512_t38k_v200"
      ],
      "metadata": {
        "id": "w39q47ngfGbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define paths\n",
        "pim_path = '/content/drive/MyDrive/superAI_lv2/hackathon/liver_detec_ultrasound/512x512'  # Adjust this path if needed\n",
        "\n",
        "# Define categories\n",
        "categories = ['machine_images', 'mobile_images']\n",
        "\n",
        "# Process each category\n",
        "for category in categories:\n",
        "    category_path = os.path.join(pim_path, category)\n",
        "    image_files = [f for f in os.listdir(category_path) if os.path.isfile(os.path.join(category_path, f))]\n",
        "\n",
        "    # Split data into 95% train and 5% test\n",
        "    train_files, val_files = train_test_split(image_files, test_size=0.05, shuffle=True, random_state=888)\n",
        "    kind = 'machine' if 'machine' in category else 'mobile'\n",
        "\n",
        "    # Resize and move train images\n",
        "    for i, file_name in tqdm(enumerate(train_files)):\n",
        "        src_path = os.path.join(category_path, file_name)\n",
        "        dst_path = os.path.join('/content/ganset512_t38k_v200/train/', kind, file_name)\n",
        "\n",
        "        image = cv2.imread(src_path, cv2.COLOR_BGR2RGB)\n",
        "        image = cv2.resize(image, (512, 512))\n",
        "        cv2.imwrite(dst_path, image)\n",
        "\n",
        "    # Resize and move test images\n",
        "    for i, file_name in tqdm(enumerate(val_files)):\n",
        "        src_path = os.path.join(category_path, file_name)\n",
        "        dst_path = os.path.join('/content/ganset512_t38k_v200/val/', kind, file_name)\n",
        "\n",
        "        image = cv2.imread(src_path, cv2.COLOR_BGR2RGB)\n",
        "        image = cv2.rsieze(image, (512, 512))\n",
        "        cv2.imwrite(dst_path, image)\n",
        "\n",
        "print(\"Data restructuring and resizing complete.\")"
      ],
      "metadata": {
        "id": "x7xOP9CHYQY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Define the folder to be zipped and the output zip file path\n",
        "folder_to_zip = f'/content/ganset512_t38k_v200'\n",
        "output_zip_file = f'/content/drive/MyDrive/superAI_lv2/hackathon/liver_detec_ultrasound/ganset512_t38k_v200'\n",
        "\n",
        "# Create a zip file from the folder\n",
        "shutil.make_archive(output_zip_file, 'zip', folder_to_zip)\n",
        "\n",
        "print(\"Folder zipped successfully!\")"
      ],
      "metadata": {
        "id": "CucX6oIWviM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gan_dataset"
      ],
      "metadata": {
        "id": "SRQUQ6D3eq7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_tag(string):\n",
        "  return string.split('.')[0]\n",
        "\n",
        "mapping_df = pd.read_csv('/content/mapping2.csv')\n",
        "mapping_df['image_tag'] = mapping_df['Image File'].apply(get_image_tag)\n",
        "mapping_df"
      ],
      "metadata": {
        "id": "49Eu2ICXFRy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "machine_df = mapping_df[mapping_df.Source == 'machine']\n",
        "mobile_df = mapping_df[mapping_df.Source == 'mobile']\n",
        "machine_df.shape, mobile_df.shape"
      ],
      "metadata": {
        "id": "QYhRhC-OINup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_process(image):\n",
        "  if len(image.shape) == 3:\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "  else:\n",
        "    gray_image = image\n",
        "\n",
        "  # blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0.2)\n",
        "\n",
        "  # clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "  # clahe_image = clahe.apply(blurred_image)\n",
        "  return gray_image"
      ],
      "metadata": {
        "id": "9Bof958KBx8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_file(image_tag):\n",
        "\n",
        "  image_path = f'/content/train/train/images/{image_tag}.jpg'\n",
        "  image = cv2.imread(image_path, cv2.COLOR_BGR2RGB)\n",
        "  if type(image) == type(None):\n",
        "    image_path = f'/content/val/val/images/{image_tag}.jpg'\n",
        "    image = cv2.imread(image_path, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  processed_image = image_process(image)\n",
        "  kind = 'train' if 'train' in image_path else 'val'\n",
        "\n",
        "  #Write image\n",
        "  image_outpath = f\"/content/gan_dataset/{kind}/mobile/{image_tag}.jpg\"\n",
        "  cv2.imwrite(image_outpath, image)\n",
        "\n",
        "  #Write text\n",
        "  # annotation_path = f'/content/{kind}/{kind}/annotations/{image_tag}.txt'\n",
        "  # destination_folder = f'/content/smooth_clahe/{kind}/labels/{image_tag}.txt'\n",
        "\n",
        "  # try:\n",
        "  #   shutil.copy(annotation_path, destination_folder)\n",
        "  # except:\n",
        "  #   print(image_tag)\n",
        "\n",
        "print(\"Train process successfully!\")"
      ],
      "metadata": {
        "id": "ZVcOAef9Bx1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "machine_df.image_tag.apply(make_file)"
      ],
      "metadata": {
        "id": "3dGBro82NAV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mobile_df.image_tag.apply(make_file)"
      ],
      "metadata": {
        "id": "1ZLfIJq2EK7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def count_files_in_directory(directory_path):\n",
        "    # List all items in the directory\n",
        "    all_items = os.listdir(directory_path)\n",
        "    # Filter out only files (not directories)\n",
        "    files = [item for item in all_items if os.path.isfile(os.path.join(directory_path, item))]\n",
        "    return len(files)\n",
        "\n",
        "# Example usage\n",
        "directory_path = '/content/drive/MyDrive/superAI_lv2/hackathon/liver_detec_ultrasound/512x512/machine_images'\n",
        "print(f\"Number of files: {count_files_in_directory(directory_path)}\")\n",
        "\n",
        "directory_path = '/content/drive/MyDrive/superAI_lv2/hackathon/liver_detec_ultrasound/512x512/mobile_images'\n",
        "print(f\"Number of files: {count_files_in_directory(directory_path)}\")"
      ],
      "metadata": {
        "id": "AQOsfN4QROkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Define the folder to be zipped and the output zip file path\n",
        "folder_to_zip = f'/content/gan_dataset'\n",
        "output_zip_file = f'/content/drive/MyDrive/superAI_lv2/hackathon/liver_detec_ultrasound/gan_dataset'\n",
        "\n",
        "# Create a zip file from the folder\n",
        "shutil.make_archive(output_zip_file, 'zip', folder_to_zip)\n",
        "\n",
        "print(\"Folder zipped successfully!\")"
      ],
      "metadata": {
        "id": "B1y0z3vKXpyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rx3vFgmLYC-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_kSvRSBa0RRj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}