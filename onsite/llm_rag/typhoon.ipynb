{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mSPt-1e0k79d",
        "sKMr75itBQdC",
        "ZGR_gCr6CMbY",
        "Rl9VaCoTcqgD",
        "tUIvYqmpgWBM"
      ],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# load data"
      ],
      "metadata": {
        "id": "mSPt-1e0k79d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'openthaigpt-exercise-ungraded:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F75666%2F8360884%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240429%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240429T035040Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D8c0f4c54bd9b0d87db02da59f4a714441ef73018ee86a6575d5a2fb9d7e63b732ad8f91d9560c9aff8a1a23b444ca6a9301c70226438052d9637c37b22e96ef096ec9370e9ef0f46059f8c09cc0bd4e95ceb835ce0a2f0d7bf66e68bb351eb06725b9e3ef2c7e37f02e9a8225f86f990baf6711ba218688ca354d765dd0a9deae2c31ea3ed254bd67829685c3536d926e7899ea92ff44b5b0ebf31f87a1fc991b9223fe9d4fe92727544ea8009dc5d5e8eac9369f4bc75f89478fb2006956b474279254473a102bca03524f7ffd9231bdca652b0850dfdaf838432b5ab5801eda46304d55a707c049cc765945da650d81462a9e5bea335b64e751884691178ff'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')"
      ],
      "metadata": {
        "id": "hcC5_4hX-pKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "tbl4 = pd.read_csv('/kaggle/input/openthaigpt-exercise-ungraded/TBL4-Online-Shopping-Dataset.csv')\n",
        "# data = pd.read_csv('/kaggle/input/openthaigpt-exercise-ungraded/data.csv')\n",
        "# spsm = pd.read_csv('/kaggle/input/openthaigpt-exercise-ungraded/sample_submission.csv')"
      ],
      "metadata": {
        "id": "_BM4Hxk9lAe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# play openai (docs typhoon: Stream support)"
      ],
      "metadata": {
        "id": "sKMr75itBQdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "PhGNfiea-p8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "OPENTYPHOON_API_KEY = \"sk-HUs9gGipoQcCQ4y3H2l81iTI2J3J6xjEFRxdHLSUUUM76rCn\"  # add your API key here\n",
        "os.environ[\"OPENTYPHOON_API_KEY\"] = OPENTYPHOON_API_KEY"
      ],
      "metadata": {
        "id": "7_VG03Z7AGQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Stream support"
      ],
      "metadata": {
        "id": "jKYiKAkWKALM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=os.environ.get(\"OPENTYPHOON_API_KEY\"),\n",
        "    base_url=\"https://api.opentyphoon.ai/v1\",\n",
        ")\n",
        "\n",
        "stream = client.chat.completions.create(\n",
        "    model=\"typhoon-instruct\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"ขอสูตรไก่ย่างหน่อย\",\n",
        "        }\n",
        "    ],\n",
        "    max_tokens=300,\n",
        "    temperature=0.6,\n",
        "    top_p=1,\n",
        "    stream=True,\n",
        ")\n",
        "\n",
        "for chunk in stream:\n",
        "    print(chunk)"
      ],
      "metadata": {
        "id": "nLCF8EsD-EEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk.choices"
      ],
      "metadata": {
        "id": "TFy560zuBZCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cm7Vg5kcCMHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# play llamaIndex"
      ],
      "metadata": {
        "id": "ZGR_gCr6CMbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index"
      ],
      "metadata": {
        "id": "nwCAsTHqBY_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install llama-index-embeddings-openai"
      ],
      "metadata": {
        "id": "4YeZ3PZwBY7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-HUs9gGipoQcCQ4y3H2l81iTI2J3J6xjEFRxdHLSUUUM76rCn\""
      ],
      "metadata": {
        "id": "WQXHwDLeDRzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get API key and create embeddings\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
        "\n",
        "# embeddings = embed_model.get_text_embedding(\n",
        "#     \"Open AI new Embeddings models is great.\"\n",
        "# )\n",
        "\n",
        "# error na ja"
      ],
      "metadata": {
        "id": "M4M-q2z6DRrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# play transformer"
      ],
      "metadata": {
        "id": "635NRepnGT16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q bitsandbytes"
      ],
      "metadata": {
        "id": "yhC72u62be4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "gyUB2QvyHxBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load model"
      ],
      "metadata": {
        "id": "Rl9VaCoTcqgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"scb10x/typhoon-7b\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"scb10x/typhoon-7b\",\n",
        "    return_dict=True,\n",
        "    load_in_4bit=True ,\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=torch.float16,\n",
        "    offload_folder=\"./\")"
      ],
      "metadata": {
        "id": "ykoXkVbZGO-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "id": "htyzyAjNfMgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "zDsdmclTfdU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config"
      ],
      "metadata": {
        "id": "eWbu_PVxfQbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## try to inference"
      ],
      "metadata": {
        "id": "bQ_KTnY-fgxh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### basic"
      ],
      "metadata": {
        "id": "tUIvYqmpgWBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"จากประโยค 'I will be right back.'\\nช่วยแปลประโยคนี้ให้เป็นภาษาอังกฤษให้หน่อย\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generate\n",
        "generate_ids = model.generate(inputs.input_ids, max_length=100)\n",
        "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
      ],
      "metadata": {
        "id": "A8RFZmVbcCcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- instruct ไทยผสมeng"
      ],
      "metadata": {
        "id": "OhXoLWGvqho5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# llama_prompt = f\"<s>[INST] <<SYS>>\\nYou are a question answering assistant. Answer the question as truthful and helpful as possible คุณคือผู้ช่วยตอบคำถาม จงตอบคำถามอย่างถูกต้องและมีประโยชน์ที่สุด<</SYS>>\\n\\n{prompt} [/INST]\"\n",
        "prompt_template = \"\"\"[INST] <<SYS>\n",
        "You are a question answering assistant. Answer the question as truthful and helpful as possible. คุณคือผู้ช่วยตอบคำถาม จงตอบคำถามอย่างถูกต้องและมีประโยชน์ที่สุด\n",
        "<</SYS>>\n",
        "\n",
        "{human_prompt} [/INST]\n",
        "\"\"\"\n",
        "full_prompt = prompt_template.format(human_prompt = \"ขอสูตรแคลคูลัสที่ควรรู้หน่อย\")\n",
        "inputs = tokenizer.encode(full_prompt, return_tensors=\"pt\")\n",
        "inputs = inputs.to(\"cuda\")\n",
        "inputs"
      ],
      "metadata": {
        "id": "_sapIm-hgY5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(inputs, max_length=512, num_return_sequences=1)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "leph29g4gY2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- instruct ไทยล้วน"
      ],
      "metadata": {
        "id": "LMYNMWqZqZQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"[INST] <<SYS>\n",
        "คุณคือผู้ช่วยตอบคำถาม จงตอบคำถามอย่างถูกต้องและมีประโยชน์ที่สุด\n",
        "<</SYS>>\n",
        "\n",
        "{human_prompt} [/INST]\n",
        "\"\"\"\n",
        "full_prompt = prompt_template.format(human_prompt = \"บอกชื่อผลไม้ไทยมา10ชนิด\")\n",
        "inputs = tokenizer.encode(full_prompt, return_tensors=\"pt\")\n",
        "inputs = inputs.to(\"cuda\")\n",
        "outputs = model.generate(inputs, max_length=512, num_return_sequences=1)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "637gDBITi6yE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"[INST] <<SYS>\n",
        "You are a question answering assistant. Answer the question as truthful and helpful as possible.\n",
        "<</SYS>>\n",
        "\n",
        "{human_prompt} [/INST]\n",
        "\"\"\"\n",
        "full_prompt = prompt_template.format(human_prompt = \"please give me names of thai food.\")\n",
        "inputs = tokenizer.encode(full_prompt, return_tensors=\"pt\")\n",
        "inputs = inputs.to(\"cuda\")\n",
        "outputs = model.generate(inputs, max_length=512, num_return_sequences=1)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "lhNzlvCFrH22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### เล่นกับ data"
      ],
      "metadata": {
        "id": "e7PGN_Opg4kE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instruction_str = (\n",
        "    \"1. Convert the query to executable Python code using Pandas.\\n\"\n",
        "    \"2. The final line of code should be a Python expression that can be called with the `eval()` function.\\n\"\n",
        "    \"3. The code should represent a solution to the query.\\n\"\n",
        "    \"4. PRINT ONLY THE EXPRESSION.\\n\"\n",
        "    \"5. Do not quote the expression.\\n\"\n",
        "    \"6. Consider that, when asked about number of customer, you need to count for unique customer based on customer ID\"\n",
        "    \"\"\"Consider these fields: CustomerID\n",
        "\n",
        "Description: Unique identifier for each customer.\n",
        "\n",
        "Data Type: Numeric;\n",
        "\n",
        "Gender\n",
        "\n",
        "Description: Gender of the customer (e.g., Male, Female).\n",
        "\n",
        "Data Type: Categorical;\n",
        "\n",
        "Location\n",
        "\n",
        "Description: Location or address information of the customer.\n",
        "\n",
        "Data Type: Text;\n",
        "\n",
        "Tenure_Months\n",
        "\n",
        "Description: Number of months the customer has been associated with the platform.\n",
        "Data Type: Numeric;\n",
        "\n",
        "Transaction_ID\n",
        "\n",
        "Description: Unique identifier for each transaction.\n",
        "\n",
        "Data Type: Numeric;\n",
        "\n",
        "Transaction_Date\n",
        "\n",
        "Description: Date of the transaction.\n",
        "\n",
        "Data Type: Date;\n",
        "\n",
        "Product_SKU\n",
        "\n",
        "Description: Stock Keeping Unit (SKU) identifier for the product.\n",
        "\n",
        "Data Type: Text;\n",
        "\n",
        "Product_Description\n",
        "\n",
        "Description: Description of the product.\n",
        "\n",
        "Data Type: Text;\n",
        "\n",
        "Product_Category:\n",
        "\n",
        "Description: Category to which the product belongs.\n",
        "\n",
        "Data Type: Categorical;\n",
        "\n",
        "Quantity\n",
        "\n",
        "Description: Quantity of the product purchased in the transaction.\n",
        "\n",
        "Data Type: Numeric;\n",
        "\n",
        "Avg_Price\n",
        "\n",
        "Description: Average price of the product.\n",
        "\n",
        "Data Type: Numeric;\n",
        "\n",
        "Total_Price\n",
        "\n",
        "Description: Total price of the product exclude delivery charges.\n",
        "\n",
        "Data Type: Numeric;\n",
        "\n",
        "Delivery_Charges\n",
        "\n",
        "Description: Charges associated with the delivery of the product.\n",
        "\n",
        "Data Type: Numeric;\n",
        "\n",
        "Date\n",
        "\n",
        "Description: Date of the transaction (potentially redundant with Transaction_Date).\n",
        "\n",
        "Data Type: Date;\n",
        "\n",
        "Month\n",
        "\n",
        "Description: Month of the transaction.\n",
        "\n",
        "Data Type: Numeric; \"\"\"\n",
        ")\n",
        "\n",
        "pandas_prompt_str = (\n",
        "    \"You are working with a pandas dataframe in Python.\\n\"\n",
        "    \"The name of the dataframe is `df`.\\n\"\n",
        "    \"This is the result of `print(df.head())`:\\n\"\n",
        "    \"{df_str}\\n\\n\"\n",
        "    \"Follow these instructions:\\n\"\n",
        "    \"{instruction_str}\\n\"\n",
        "    \"Query: {query_str}\\n\\n\"\n",
        "    \"Expression:\"\n",
        ")\n",
        "\n",
        "response_synthesis_prompt_str = (\n",
        "    \"Given an input question, synthesize a response from the query results.\\n\"\n",
        "    \"Query: {query_str}\\n\\n\"\n",
        "    \"If the answer is a number, simply state the number, DO NOT add any other text\"\n",
        "    \"Example: Do this : 100\\nDo NOT DO this: The number of customer is 100.\"\n",
        "    \"If there're more than 2 decimal points, only print the first two.\"\n",
        "    \"Pandas Instructions (optional):\\n{pandas_instructions}\\n\\n\"\n",
        "    \"Pandas Output: {pandas_output}\\n\\n\"\n",
        "    \"Response: \"\n",
        ")"
      ],
      "metadata": {
        "id": "yqkrPyp4uO4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the average quantity of products purchased per transaction in october, please answer the number in 2 decimal places?\""
      ],
      "metadata": {
        "id": "CWxGXPQMgYyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pandas_prompt = pandas_prompt_str.format(df_str = tbl4.head(), instruction_str = instruction_str, query_str = query)"
      ],
      "metadata": {
        "id": "lKV9RvdUHqIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pandas_prompt"
      ],
      "metadata": {
        "id": "yr5NEwFqwoXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"[INST] <<SYS>\n",
        "You are a question answering assistant. Answer the question as truthful and helpful as possible คุณคือผู้ช่วยตอบคำถาม จงตอบคำถามอย่างถูกต้องและมีประโยชน์ที่สุด\n",
        "<</SYS>>\n",
        "\n",
        "{human_prompt} [/INST]\n",
        "\"\"\"\n",
        "full_prompt = prompt_template.format(human_prompt = pandas_prompt)\n",
        "\n",
        "tokens = tokenizer(full_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "output = model.generate(tokens[\"input_ids\"], max_new_tokens=256, temperature = 0.55)\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "lnbAEoOOy7dY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"### USER:\\n{human_prompt}\\n\\n### RESPONSE:\\n\"\n",
        "prompt = \"\"\"Write SQL code\"\"\"\n",
        "full_prompt = prompt_template.format(human_prompt=pandas_prompt)\n",
        "\n",
        "tokens = tokenizer(full_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "output = model.generate(tokens[\"input_ids\"], max_new_tokens=256, temperature = 0.55)\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "z8RtzxLa0cPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"### USER:\\n{human_prompt}\\n\\n### RESPONSE:\\n\"\n",
        "prompt = \"\"\"You are working with tabular data.\n",
        "This is a part of the table named test1\n",
        "   CustomerID Gender Location  Tenure_Months  Transaction_ID Transaction_Date  \\\n",
        "0       17850      M  Chicago             12           16679       2019-01-01\n",
        "1       17850      M  Chicago             12           16680       2019-01-01\n",
        "2       17850      M  Chicago             12           16696       2019-01-01\n",
        "3       17850      M  Chicago             12           16699       2019-01-01\n",
        "4       17850      M  Chicago             12           16700       2019-01-01\n",
        "\n",
        "      Product_SKU                                Product_Description  \\\n",
        "0  GGOENEBJ079499  Nest Learning Thermostat 3rd Gen-USA - Stainle...\n",
        "1  GGOENEBJ079499  Nest Learning Thermostat 3rd Gen-USA - Stainle...\n",
        "2  GGOENEBQ078999             Nest Cam Outdoor Security Camera - USA\n",
        "3  GGOENEBQ079099    Nest Protect Smoke + CO White Battery Alarm-USA\n",
        "4  GGOENEBJ079499  Nest Learning Thermostat 3rd Gen-USA - Stainle...\n",
        "\n",
        "  Product_Category  Quantity  Avg_Price  Total_Price  Delivery_Charges  \\\n",
        "0         Nest-USA         1     153.71       153.71               6.5\n",
        "1         Nest-USA         1     153.71       153.71               6.5\n",
        "2         Nest-USA         2     122.77       245.54               6.5\n",
        "3         Nest-USA         1      81.50        81.50               6.5\n",
        "4         Nest-USA         1     153.71       153.71               6.5\n",
        "\n",
        "       Date  Month\n",
        "0  1/1/2019      1\n",
        "1  1/1/2019      1\n",
        "2  1/1/2019      1\n",
        "3  1/1/2019      1\n",
        "4  1/1/2019      1\n",
        "Write SQL code to answer the question: What is the average quantity of products purchased per transaction in october, please answer the number in 2 decimal places?\"\"\"\n",
        "prompt_template = \"\"\"[INST] <<SYS>\n",
        "คุณคือผู้ช่วยตอบคำถาม จงตอบคำถามอย่างถูกต้องและมีประโยชน์ที่สุด\n",
        "<</SYS>>\n",
        "\n",
        "{human_prompt} [/INST]\n",
        "\"\"\"\n",
        "full_prompt = prompt_template.format(human_prompt = prompt)\n",
        "\n",
        "tokens = tokenizer(full_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "output = model.generate(tokens[\"input_ids\"], max_new_tokens=256)\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "V8nU2Byv17-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-yLRoMOz177P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_gu6gfV8_MY"
      },
      "outputs": [],
      "source": []
    }
  ]
}