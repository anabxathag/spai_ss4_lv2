{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/superAI_lv2/hackathon/Final_5hacks/nithan_chadok/chadok-hybrid-ocr-ner.zip"
      ],
      "metadata": {
        "id": "Bl4xs6Yjk28E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q pytesseract"
      ],
      "metadata": {
        "id": "NzLok70IoE72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! sudo apt-get install tesseract-ocr-tha\n",
        "! sudo tesseract --list-langs"
      ],
      "metadata": {
        "id": "gvoafvmaoE4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# process"
      ],
      "metadata": {
        "id": "CTO7ikU0osu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CamembertTokenizerFast, CamembertForTokenClassification, pipeline\n",
        "# from transformers import RobertaTokenizerFast, RobertaForTokenClassification, pipeline\n",
        "\n",
        "# tokenizer = CamembertTokenizerFast.from_pretrained(\"thanaphatt1/WangchanBERTa-LST20\")\n",
        "# model = CamembertForTokenClassification.from_pretrained(\"thanaphatt1/WangchanBERTa-LST20\")\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(\"lst-nectec/HoogBERTa-NER-lst20\")\n",
        "model = RobertaForTokenClassification.from_pretrained(\"lst-nectec/HoogBERTa-NER-lst20\")\n",
        "\n",
        "nlp = pipeline('token-classification', model=model, tokenizer=tokenizer, aggregation_strategy=\"none\", ignore_labels=[], device=0)"
      ],
      "metadata": {
        "id": "n7gmChB1osJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "import math\n",
        "import pytesseract\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "1gAVM-n1pQTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_list = pd.read_csv('tag_list.csv')\n",
        "tags = {row['tag']:row['class'] for _, row in tag_list.iterrows()}\n",
        "kernel = np.array([\n",
        "    [0, 0, 0],\n",
        "    [0, 1, 0],\n",
        "    [0,0,0]\n",
        "])\n",
        "preds = []"
      ],
      "metadata": {
        "id": "R0PwuASUpQPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "image = Image.open('images/images/00001.jpg')\n",
        "image"
      ],
      "metadata": {
        "id": "KJw_CSCqqORZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_tesseract = pytesseract.image_to_string(image, lang='tha')\n",
        "print(output_tesseract)"
      ],
      "metadata": {
        "id": "FUcMrzdfqOOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getSkewAngle(cvImage) -> float:\n",
        "    # Prep image, copy, convert to gray scale, blur, and threshold\n",
        "    newImage = np.array(cvImage)\n",
        "    gray = cv2.cvtColor(newImage, cv2.COLOR_BGR2GRAY)\n",
        "    blur = cv2.GaussianBlur(gray, (9, 9), 0)\n",
        "    thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
        "\n",
        "    # Apply dilate to merge text into meaningful lines/paragraphs.\n",
        "    # Use larger kernel on X axis to merge characters into single line, cancelling out any spaces.\n",
        "    # But use smaller kernel on Y axis to separate between different blocks of text\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (30, 5))\n",
        "    dilate = cv2.dilate(thresh, kernel, iterations=2)\n",
        "\n",
        "    # Find all contours\n",
        "    contours, hierarchy = cv2.findContours(dilate, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    contours = sorted(contours, key = cv2.contourArea, reverse = True)\n",
        "    firstbox = []\n",
        "    lastbox = []\n",
        "    box = []\n",
        "    for c in contours:\n",
        "        rect = cv2.boundingRect(c)\n",
        "        x,y,w,h = rect\n",
        "        box.append([x,y,x+w,y+h])\n",
        "        cv2.rectangle(newImage,(x,y),(x+w,y+h),(0,255,0),2)\n",
        "\n",
        "    s_box = sorted(box, key = lambda x: x[1])\n",
        "    f_box = s_box[0]\n",
        "    l_box = s_box[-1]\n",
        "    if -f_box[0]+l_box[0] == 0:\n",
        "        angle = 0\n",
        "    else:\n",
        "        m = (-l_box[1]+f_box[1])/(-f_box[0]+l_box[0])\n",
        "        degree = math.atan(m)\n",
        "        angle = degree * 180 / math.pi\n",
        "\n",
        "    # Find largest contour and surround in min area box\n",
        "    largestContour = contours[0]\n",
        "    minAreaRect = cv2.minAreaRect(largestContour)\n",
        "#     cv2.imwrite(\"boxes.jpg\", newImage)\n",
        "    if angle <0:\n",
        "        angle += 90\n",
        "    elif angle > 0:\n",
        "        angle -= 90\n",
        "\n",
        "    return angle"
      ],
      "metadata": {
        "id": "xj49xVBbqOLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "getSkewAngle(image)"
      ],
      "metadata": {
        "id": "GWi8RtOdosGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rotate the image around its center\n",
        "def rotateImage(cvImage, angle: float):\n",
        "    newImage = np.array(cvImage)\n",
        "    (h, w) = newImage.shape[:2]\n",
        "    center = (w // 2, h // 2)\n",
        "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
        "    newImage = cv2.warpAffine(newImage, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
        "    return newImage\n",
        "def deskew(cvImage):\n",
        "    angle = getSkewAngle(cvImage)\n",
        "    return rotateImage(cvImage, -1.0 * angle)"
      ],
      "metadata": {
        "id": "odus6e4AoW6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(deskew(image))"
      ],
      "metadata": {
        "id": "D-uwu98atooA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image = Image.open('/kaggle/input/nithan-chadok-hybrid-ocr-ner/images/images/00000.jpg')\n",
        "inputcopy = deskew(image)\n",
        "grayinput = cv2.cvtColor(inputcopy, cv2.COLOR_BGR2GRAY)\n",
        "_, binaryImage = cv2.threshold(grayinput, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "kernelSize = (5, 5)\n",
        "opIterations = 2\n",
        "morphKernel = cv2.getStructuringElement(cv2.MORPH_RECT, kernelSize)\n",
        "dilateImage = cv2.morphologyEx(\n",
        "    binaryImage,\n",
        "    cv2.MORPH_DILATE,\n",
        "    morphKernel,\n",
        "    None,\n",
        "    None,\n",
        "    opIterations,\n",
        "    cv2.BORDER_REFLECT101\n",
        ")\n",
        "\n",
        "contours, _ = cv2.findContours(dilateImage, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "mean_row = np.array([c[:, 0].mean(0)[1] for c in contours])\n",
        "min_row = np.array([c[:, 0].min(0)[1] for c in contours], dtype='int')\n",
        "max_row = np.array([c[:, 0].max(0)[1] for c in contours], dtype='int')\n",
        "min_col = np.array([c[:, 0].min(0)[0] for c in contours], dtype='int')\n",
        "max_col = np.array([c[:, 0].max(0)[0] for c in contours], dtype='int')\n",
        "row_idx = np.argsort(mean_row)\n",
        "words = []\n",
        "\n",
        "for idx in range(len(row_idx)):\n",
        "    xx = cv2.drawContours(np.zeros_like(image, dtype='uint8'), contours, row_idx[idx], (1, 1, 1), thickness=-1)\n",
        "    im = (1 - inputcopy) * xx\n",
        "    im = im[min_row[row_idx[idx]]:max_row[row_idx[idx]], min_col[row_idx[idx]]:max_col[row_idx[idx]]]\n",
        "    gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
        "    sharpened = cv2.filter2D(1 - gray, -1, kernel)\n",
        "\n",
        "    thresh1 = (gray < 100).astype('uint8') * 255\n",
        "    thresh2 = cv2.adaptiveThreshold(sharpened, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 21, 10)\n",
        "    padded = cv2.copyMakeBorder(thresh1.copy(), 10, 10, 10, 10, cv2.BORDER_CONSTANT,value=(255,))\n",
        "    word = pytesseract.image_to_string(padded, lang='tha').replace(\"_\",\"[!und:]\").replace(\" \",\"[!und:]\").replace('\\x0c', '').replace('\\n', '')\n",
        "    print(word)\n",
        "    if len(word) == 0:\n",
        "        word = '[!und:]'\n",
        "    words.append(word)\n",
        "    preds.append(nlp(word)[0]['entity'])\n",
        "words, preds"
      ],
      "metadata": {
        "id": "JZbuzs0qqmMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## read each word"
      ],
      "metadata": {
        "id": "wxpsOMSGV20Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytesseract\n",
        "\n",
        "def crop_image(image):\n",
        "    grayinput = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    _, binaryImage = cv2.threshold(grayinput, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "    kernelSize = (5, 5)\n",
        "    opIterations = 2\n",
        "    morphKernel = cv2.getStructuringElement(cv2.MORPH_RECT, kernelSize)\n",
        "    dilateImage = cv2.morphologyEx(\n",
        "        binaryImage,\n",
        "        cv2.MORPH_DILATE,\n",
        "        morphKernel,\n",
        "        None,\n",
        "        None,\n",
        "        opIterations,\n",
        "        cv2.BORDER_REFLECT101\n",
        "        )\n",
        "    contours, _ = cv2.findContours(dilateImage, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    mean_row = np.array([c[:, 0].mean(0)[1] for c in contours])\n",
        "    min_row = np.array([c[:, 0].min(0)[1] for c in contours], dtype='int')\n",
        "    max_row = np.array([c[:, 0].max(0)[1] for c in contours], dtype='int')\n",
        "    min_col = np.array([c[:, 0].min(0)[0] for c in contours], dtype='int')\n",
        "    max_col = np.array([c[:, 0].max(0)[0] for c in contours], dtype='int')\n",
        "    row_idx = np.argsort(mean_row)\n",
        "    return row_idx, contours, min_row, max_row, min_col, max_col\n",
        "\n",
        "def image_to_words(row_idx, contours, min_row, max_row, min_col, max_col, image):\n",
        "    words = []\n",
        "    for idx in range(len(row_idx)):\n",
        "        im = image[min_row[row_idx[idx]]:max_row[row_idx[idx]], min_col[row_idx[idx]]:max_col[row_idx[idx]]]\n",
        "        word = pytesseract.image_to_string(im, lang='tha').replace(\"_\",\"[!und:]\").replace(\" \",\"[!und:]\").replace('\\x0c', '').replace('\\n', '')\n",
        "        if len(word) == 0:\n",
        "            word = '-'\n",
        "        words.append(word)\n",
        "    return words"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-09T10:47:51.538605Z",
          "iopub.execute_input": "2024-03-09T10:47:51.539548Z",
          "iopub.status.idle": "2024-03-09T10:47:51.550572Z",
          "shell.execute_reply.started": "2024-03-09T10:47:51.539509Z",
          "shell.execute_reply": "2024-03-09T10:47:51.549656Z"
        },
        "trusted": true,
        "id": "ACdHaqb4V20Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = Image.open(\"images/images/00001.jpg\")\n",
        "image = np.array(image)\n",
        "inputcopy = deskew(image)\n",
        "row_idx, contours, min_row, max_row, min_col, max_col = crop_image(inputcopy)\n",
        "\n",
        "num_rows = 2\n",
        "num_cols = 7\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(10, 3))\n",
        "\n",
        "for idx, ax in enumerate(axes.flat):\n",
        "    if idx < len(row_idx):\n",
        "        im = inputcopy[min_row[row_idx[idx]]:max_row[row_idx[idx]], min_col[row_idx[idx]]:max_col[row_idx[idx]]]\n",
        "        ax.imshow(im, cmap='gray')\n",
        "        ax.axis('off')\n",
        "    else:\n",
        "        ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-09T10:48:23.216108Z",
          "iopub.execute_input": "2024-03-09T10:48:23.216536Z",
          "iopub.status.idle": "2024-03-09T10:48:23.76135Z",
          "shell.execute_reply.started": "2024-03-09T10:48:23.216508Z",
          "shell.execute_reply": "2024-03-09T10:48:23.760064Z"
        },
        "trusted": true,
        "id": "qJMLoyVSV20Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = image_to_words(row_idx, contours, min_row, max_row, min_col, max_col, image)\n",
        "print(words)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-09T10:45:04.227833Z",
          "iopub.execute_input": "2024-03-09T10:45:04.228245Z",
          "iopub.status.idle": "2024-03-09T10:45:04.832188Z",
          "shell.execute_reply.started": "2024-03-09T10:45:04.228204Z",
          "shell.execute_reply": "2024-03-09T10:45:04.831005Z"
        },
        "trusted": true,
        "id": "ZGIDCeaTV20Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# submit"
      ],
      "metadata": {
        "id": "REWaZ9UjV20Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## transform & load data"
      ],
      "metadata": {
        "id": "8gYppRx7V20Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src = 'images/images'\n",
        "txt = []\n",
        "\n",
        "for i in tqdm(range(len(os.listdir(src)))):\n",
        "    image = Image.open(f'{src}/{i:05d}.jpg')\n",
        "    image = np.array(image)\n",
        "    inputcopy = deskew(image)\n",
        "    row_idx, contours, min_row, max_row, min_col, max_col = crop_image(inputcopy)\n",
        "    words = image_to_words(row_idx, contours, min_row, max_row, min_col, max_col, inputcopy)\n",
        "    txt.extend(words)\n",
        "\n",
        "print(len(txt))"
      ],
      "metadata": {
        "trusted": true,
        "id": "_qiiBDhkV20Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({'Text': txt})\n",
        "df.to_csv('text_data.csv', index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-08T20:07:46.927435Z",
          "iopub.execute_input": "2024-03-08T20:07:46.927861Z",
          "iopub.status.idle": "2024-03-08T20:07:46.977066Z",
          "shell.execute_reply.started": "2024-03-08T20:07:46.927829Z",
          "shell.execute_reply": "2024-03-08T20:07:46.975945Z"
        },
        "trusted": true,
        "id": "g1_8lQI1V20Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## read test data"
      ],
      "metadata": {
        "id": "ddyVZirsV20Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NER_TAGS = [\n",
        "       \"O\",\n",
        "        \"B_BRN\",        \"B_DES\",        \"B_DTM\",        \"B_LOC\",        \"B_MEA\",        \"B_NUM\",        \"B_ORG\",        \"B_PER\",        \"B_TRM\",        \"B_TTL\",\n",
        "       \"I_BRN\",        \"I_DES\",        \"I_DTM\",        \"I_LOC\",        \"I_MEA\",        \"I_NUM\",        \"I_ORG\",        \"I_PER\",        \"I_TRM\",        \"I_TTL\",\n",
        "        \"E_BRN\",        \"E_DES\",        \"E_DTM\",        \"E_LOC\",        \"E_MEA\",        \"E_NUM\",        \"E_ORG\",        \"E_PER\",        \"E_TRM\",        \"E_TTL\"]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-08T23:24:49.224925Z",
          "iopub.execute_input": "2024-03-08T23:24:49.225312Z",
          "iopub.status.idle": "2024-03-08T23:24:49.230811Z",
          "shell.execute_reply.started": "2024-03-08T23:24:49.225286Z",
          "shell.execute_reply": "2024-03-08T23:24:49.22984Z"
        },
        "trusted": true,
        "id": "ySfQwRSMV20Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt = pd.read_csv('test.csv')\n",
        "txt = txt['word']\n",
        "txt"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-08T23:24:54.418097Z",
          "iopub.execute_input": "2024-03-08T23:24:54.419074Z",
          "iopub.status.idle": "2024-03-08T23:24:54.450504Z",
          "shell.execute_reply.started": "2024-03-08T23:24:54.419043Z",
          "shell.execute_reply": "2024-03-08T23:24:54.449539Z"
        },
        "trusted": true,
        "id": "OLFKEtXaV20a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_list = []\n",
        "for t in txt:\n",
        "  text_list.append(\" \".join(t))\n",
        "text_list"
      ],
      "metadata": {
        "id": "UECGFU4xDz1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## read model"
      ],
      "metadata": {
        "id": "LHIp55W3V20a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizerFast, RobertaForTokenClassification, pipeline\n",
        "\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(\"lst-nectec/HoogBERTa-NER-lst20\")\n",
        "model = RobertaForTokenClassification.from_pretrained(\"lst-nectec/HoogBERTa-NER-lst20\")\n",
        "# Define the model and tokenizer\n",
        "nlp = pipeline('token-classification', model=model, tokenizer=tokenizer, aggregation_strategy=\"none\", ignore_labels=[])\n",
        "nlp"
      ],
      "metadata": {
        "id": "knXtjOg01H-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## token & prediction"
      ],
      "metadata": {
        "id": "6dBCUSEqV20b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = []\n",
        "count = 0\n",
        "\n",
        "raw_text = []\n",
        "\n",
        "for text in tqdm(txt):\n",
        "    # Split the text into words.\n",
        "    text_follow_up = text.split()\n",
        "\n",
        "    # Predict the text.\n",
        "    pred = nlp(text)\n",
        "    count += len(pred)\n",
        "    # This below code is to handle the case that the text is not in the same order as the prediction.\n",
        "    current = 0\n",
        "    batch = []\n",
        "    coming_text = \"\"\n",
        "\n",
        "    while len(pred):\n",
        "        # If the text is the same, we will append the entity.\n",
        "        if pred[0]['word'].replace('</w>', '') == text_follow_up[current]:\n",
        "            raw_text.append(pred[0]['word'])\n",
        "            batch.append(pred[0]['entity'])\n",
        "            pred.pop(0)\n",
        "            current += 1\n",
        "        else:\n",
        "            # If the text is not the same, we will append the text to the coming_text.\n",
        "            if coming_text + pred[0]['word'].replace('</w>', '') == text_follow_up[current]:\n",
        "                raw_text.append(pred[0]['word'])\n",
        "                batch.append(pred[0]['entity'])\n",
        "                pred.pop(0)\n",
        "                current += 1\n",
        "                coming_text = \"\"\n",
        "                continue\n",
        "\n",
        "            coming_text += pred[0]['word']\n",
        "            pred.pop(0)\n",
        "\n",
        "    preds.extend(batch)"
      ],
      "metadata": {
        "id": "FWcaTARPDT33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(preds).value_counts()"
      ],
      "metadata": {
        "id": "sBt3sfKhFZeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_df = pd.read_csv('tag_list.csv')\n",
        "tags = {row['tag']:row['class'] for _, row in tag_df.iterrows()}\n",
        "tags"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-08T23:25:52.044251Z",
          "iopub.execute_input": "2024-03-08T23:25:52.044622Z",
          "iopub.status.idle": "2024-03-08T23:25:52.068407Z",
          "shell.execute_reply.started": "2024-03-08T23:25:52.044593Z",
          "shell.execute_reply": "2024-03-08T23:25:52.067467Z"
        },
        "trusted": true,
        "id": "AWn19P5IV20b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit = pd.read_csv('sample_submission.csv')"
      ],
      "metadata": {
        "id": "V7SK5SjRc7Zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit.loc[3:, 'pred'] = preds[3:]\n",
        "submit.loc[3:, 'pred'] = submit.pred.map(tags)\n",
        "submit"
      ],
      "metadata": {
        "id": "IR1lqgoJdYSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit.pred = submit.pred.astype(int)\n",
        "submit.pred.value_counts()"
      ],
      "metadata": {
        "id": "RzomyAP4e65o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit.to_csv('nlp_testinfer_HoogBERTa_NER_lst20.csv', index=False)"
      ],
      "metadata": {
        "id": "t1sMhXDjef3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yLWr013LgEnY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}